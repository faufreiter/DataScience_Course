{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c8cb76-24cb-47c2-a35d-27a2f09016e5",
   "metadata": {},
   "source": [
    "#### Source: [this article](https://medium.com/@bhanusree.balisetty/from-pandas-to-pyspark-e7188c8276e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4143e213-6ab8-4ce6-b74a-fb6ebc318e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b185e4-445a-4f6e-b446-95453a9d67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac49fa8-fb59-49d5-a9dc-57ad5cab84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/08 00:26:17 WARN Utils: Your hostname, vienna resolves to a loopback address: 127.0.1.1; using 10.0.0.32 instead (on interface enp3s0)\n",
      "22/06/08 00:26:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/renato/Documents/env_default/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/08 00:26:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('spark_session').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714aa74-b0f7-473e-8fed-f104a6d66d25",
   "metadata": {},
   "source": [
    "#### Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56bdf6d-50f8-4529-bd9f-212f5539ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df1 = [['A1', 'B1', 2, '21-12-2021 10:30'], \n",
    "       ['A2', 'B2', 4, '21-12-2021 10:40'], \n",
    "       ['A3', 'B3', 5, '21-12-2021 11:00']] \n",
    "\n",
    "df1 = pd.DataFrame(df1, columns = ['A', 'B', 'Value', 'Date_Column'])\n",
    "\n",
    "\n",
    "\n",
    "# PYSPARK\n",
    "df2 = spark.createDataFrame([('A1', 'B1', 2, '21-12-2021 10:30'),\n",
    "                            ('A2', 'B2', 4, '21-12-2021 10:40'),\n",
    "                            ('A3', 'B3', 5, '21-12-2021 11:00')],\n",
    "                            ['A', 'B', 'Value', 'Date_Column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24add0-aed3-453d-b05f-b6e167e0ddc1",
   "metadata": {},
   "source": [
    "#### Creating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac5050-4f1d-413f-9c36-3d6cc4b9ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - New column with Constant values\n",
    "df1['C'] = 'New Constant'\n",
    "\n",
    "# PYSPARK - New column with Constant values\n",
    "df2 = df2.withColumn(\"C\", F.lit('New Constant'))\n",
    "\n",
    "# PANDAS - New Column using existing columns\n",
    "df1['C'] = df1['A'] + df1['B']\n",
    "\n",
    "# PYSPARK - New Column using existing columns\n",
    "df2 = df2.withColumn(\"C\", F.concat(\"A\", \"B\"))\n",
    "\n",
    "#NOTE:\n",
    "#lit() -- used to create constant columns\n",
    "#concat() -- concatenate columns of dataframe\n",
    "#withColumn() -- creates a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe16b6-4e2d-4151-9ee1-9ad9c2dc60ea",
   "metadata": {},
   "source": [
    "#### Updating Existing Column Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27b303a-a47f-4991-8b7f-4f42c864a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - Update Column data\n",
    "df1['Value'] = df1['Value']**2\n",
    "\n",
    "# PYSPARK - Update Column data\n",
    "df2 = df2.withColumn(\"Value\", F.col(\"Value\")**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db0bba-f620-4aa3-98d9-67fe1bb780c2",
   "metadata": {},
   "source": [
    "#### Selecting, Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ab6028-866e-461e-88f3-a5249dcbdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - Selecting Columns\n",
    "new_df = df1[['B', 'C']]\n",
    "\n",
    "# PYSPARK - Selecting Columns\n",
    "new_df1 = df2.select(\"B\", \"C\")\n",
    "\n",
    "# PANDAS - Filtering rows based on condition\n",
    "new_df1 = df1[df1['Value']<5]\n",
    "\n",
    "# PYSPARK - Filtering rows based on condition\n",
    "new_df2 = df2.filter(df2.Value<5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a670a-e5b0-4be5-96b1-1ef01daa8635",
   "metadata": {},
   "source": [
    "#### Column Type Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d709fd45-651f-463e-bceb-ef996650e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - Convert Column from String to DateTime format\n",
    "df1['Date_Column'] =  pd.to_datetime(df1['Date_Column'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "# PYSPARK - Convert Column from String to Timestamp format\n",
    "df2 = df2.withColumn(\"Date_Column\", F.to_timestamp(\"Date_Column\", \"dd-MM-yyyy hh:mm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f098ce-82e5-4c6c-8aeb-dc1a02cb284e",
   "metadata": {},
   "source": [
    "#### Rename, Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40b7faf-893a-40ba-a53a-0a6488f50af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - Rename Columns\n",
    "df1 = df1.rename(columns={'A': 'Col_A', 'B': 'Col_B'})\n",
    "\n",
    "# PYSPARK - Rename Columns\n",
    "df2 = df2.withColumnRenamed(\"A\", \"Col_A\").withColumnRenamed(\"B\", \"Col_B\")\n",
    "\n",
    "# PANDAS - Drop Columns\n",
    "df1 = df1.drop(['Col_A', 'Col_B'], axis=1)\n",
    "\n",
    "# PYSPARK - Drop Columns\n",
    "df2 = df2.drop('A', 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41ba1f-67cc-4631-9149-800e1bf97396",
   "metadata": {},
   "source": [
    "#### Melt Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c4fbb8-69b8-4005-bff2-b080c63816da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A variable  value\n",
       "0  a        B      1\n",
       "1  b        B      3\n",
       "2  c        B      5\n",
       "3  a        C      2\n",
       "4  b        C      4\n",
       "5  c        C      6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PANDAS\n",
    "df1 = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                    'B': {0: 1, 1: 3, 2: 5},\n",
    "                    'C': {0: 2, 1: 4, 2: 6}})\n",
    "\n",
    "pd.melt(df1, id_vars=['A'], value_vars=['B', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8377d7-d26c-4609-8fa3-455aa677795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYSPARK custom melt function\n",
    "def melt(df, id_vars, value_vars, var_name=\"Variable\", value_name=\"Value\"):\n",
    "    _vars_and_vals = F.array(*(F.struct(F.lit(c).alias(var_name),\n",
    "                                        F.col(c).alias(value_name)) for c in value_vars))\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\",\n",
    "                         F.explode(_vars_and_vals))\n",
    "    cols = id_vars + [F.col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame([('a', 1, 2), ('b', 3, 4), ('c', 5, 6)], ['A', 'B', 'C'])\n",
    "\n",
    "#melt(df2, ['A'], ['B', 'C']).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5806e-8f24-4943-a0f8-1d063fc83c76",
   "metadata": {},
   "source": [
    "#### Add Interval to a Timestamp Column (Timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ba46ea-dadb-499f-a14f-8a3f08ef2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS - Add 'Interval' to 'Start_Time'\n",
    "df1 = pd.DataFrame([['2021-01-10 10:10:00', '00:05'],\n",
    "                    ['2021-12-10, 05:30:00', '00:15'],\n",
    "                    ['2021-11-10 11:40:00', '00:20']], \n",
    "                   columns = ['Start_Time','Interval'])\n",
    "\n",
    "df1['Start_Time'] = pd.to_datetime(df1['Start_Time'])\n",
    "df1['End_Time'] = df1['Start_Time'] + pd.to_timedelta(pd.to_datetime(df1['Interval']).dt.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cab41fd-46de-405f-8122-4338633b273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYSPARK - Add 'Interval' to 'Start_Time'\n",
    "df2 = spark.createDataFrame([['2021-01-10 10:10:00', '00:05'], \n",
    "                            ['2021-12-10 05:30:00', '00:15'], \n",
    "                            ['2021-11-10 11:40:00', '00:20']], \n",
    "                           ['Start_Time', 'Interval'])\n",
    "\n",
    "df2 = df2.withColumn(\"Start_Time\", F.to_timestamp(\"Start_Time\", \"yyyy-MM-dd hh:mm:ss\"))\n",
    "df2 = df2.withColumn(\"End_Time\", (F.unix_timestamp(\"Start_Time\") + F.unix_timestamp(\"Interval\", \"HH:mm\")).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5711f-93f0-441f-8eb2-0bcb40cf490c",
   "metadata": {},
   "source": [
    "#### Additional Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8729cdc2-4d7e-4333-8273-c6485f1d3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[A: string, B: bigint, sum(C): bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PANDAS df\n",
    "df1 = pd.DataFrame({'A': {0: 'a', 1: 'a', 2: 'c'},\n",
    "                    'B': {0: 1, 1: 1, 2: 5},\n",
    "                    'C': {0: 2, 1: 4, 2: 6}})\n",
    "\n",
    "# PYSPARK df\n",
    "df2 = spark.createDataFrame([('a', 1, 2), ('a', 1, 4), ('c', 5, 6)],\n",
    "                            ['A', 'B', 'C'])\n",
    "\n",
    "# PANDAS - Shape of dataframe\n",
    "print(df1.shape)\n",
    "\n",
    "# PYSPARK - Shape of dataframe\n",
    "print((df2.count(), len(df2.columns)))\n",
    "\n",
    "\n",
    "# PANDAS - Distinct Values of a Column\n",
    "df1['A'].unique()\n",
    "\n",
    "# PYSPARK - Distinct Values of a Column\n",
    "df2.select('A').distinct()\n",
    "\n",
    "# PANDAS - Group by Columns - Calculate Aggregate functions\n",
    "df1.groupby(['A', 'B']).sum()\n",
    "\n",
    "# PYSPARK - Group by Columns - Calculate Aggregate functions\n",
    "df2.groupBy(\"A\", \"B\").agg(F.sum(\"C\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
