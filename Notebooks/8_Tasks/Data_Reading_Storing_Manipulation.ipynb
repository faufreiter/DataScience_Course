{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Science\n",
    "\n",
    "### Saving, Accessing and Manipulating Data in different formats\n",
    "\n",
    "[Additional info csv, xml and json](https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = pathlib.Path(\"../datasets/CSVs/\")\n",
    "datapath2 = pathlib.Path('../datasets/Miscellaneous/')\n",
    "datapath3 = pathlib.Path('../datasets/Figs//')\n",
    "outputs = pathlib.Path(\"../outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputs / 'newfile.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write('Weight\\t\\t72\\n')\n",
    "    f.write('Height\\t\\t183\\n')\n",
    "    f.write('Age\\t\\t44\\n')\n",
    "    f.write('Gender\\t\\tMasculine\\n')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputs / 'newfile.txt', mode='r', encoding='utf-8') as f:\n",
    "    fulltext = f.read()\n",
    "    f.seek(0)\n",
    "    line = f.readline()\n",
    "    f.seek(0)\n",
    "    list_of_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\t\t72\n",
      "Height\t\t183\n",
      "Age\t\t44\n",
      "Gender\t\tMasculine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\t\t72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weight\\t\\t72\\n', 'Height\\t\\t183\\n', 'Age\\t\\t44\\n', 'Gender\\t\\tMasculine\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Numpy Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05  0.055\n",
      " 0.06  0.065 0.07  0.075 0.08  0.085 0.09  0.095 0.1   0.105 0.11  0.115\n",
      " 0.12  0.125 0.13  0.135 0.14  0.145 0.15  0.155 0.16  0.165 0.17  0.175\n",
      " 0.18  0.185 0.19  0.195 0.2   0.205 0.21  0.215 0.22  0.225 0.23  0.235\n",
      " 0.24  0.245 0.25  0.255 0.26  0.265 0.27  0.275 0.28  0.285 0.29  0.295\n",
      " 0.3   0.305 0.31  0.315 0.32  0.325 0.33  0.335 0.34  0.345 0.35  0.355\n",
      " 0.36  0.365 0.37  0.375 0.38  0.385 0.39  0.395 0.4   0.405 0.41  0.415\n",
      " 0.42  0.425 0.43  0.435 0.44  0.445 0.45  0.455 0.46  0.465 0.47  0.475\n",
      " 0.48  0.485 0.49  0.495 0.5   0.505 0.51  0.515 0.52  0.525 0.53  0.535\n",
      " 0.54  0.545 0.55  0.555 0.56  0.565 0.57  0.575 0.58  0.585 0.59  0.595\n",
      " 0.6   0.605 0.61  0.615 0.62  0.625 0.63  0.635 0.64  0.645 0.65  0.655\n",
      " 0.66  0.665 0.67  0.675 0.68  0.685 0.69  0.695 0.7   0.705 0.71  0.715\n",
      " 0.72  0.725 0.73  0.735 0.74  0.745 0.75  0.755 0.76  0.765 0.77  0.775\n",
      " 0.78  0.785 0.79  0.795 0.8   0.805 0.81  0.815 0.82  0.825 0.83  0.835\n",
      " 0.84  0.845 0.85  0.855 0.86  0.865 0.87  0.875 0.88  0.885 0.89  0.895\n",
      " 0.9   0.905 0.91  0.915 0.92  0.925 0.93  0.935 0.94  0.945 0.95  0.955\n",
      " 0.96  0.965 0.97  0.975 0.98  0.985 0.99  0.995 1.   ]\n"
     ]
    }
   ],
   "source": [
    "data = np.linspace(0,1,201)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(outputs / 'data.dat', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05  0.055\n",
      " 0.06  0.065 0.07  0.075 0.08  0.085 0.09  0.095 0.1   0.105 0.11  0.115\n",
      " 0.12  0.125 0.13  0.135 0.14  0.145 0.15  0.155 0.16  0.165 0.17  0.175\n",
      " 0.18  0.185 0.19  0.195 0.2   0.205 0.21  0.215 0.22  0.225 0.23  0.235\n",
      " 0.24  0.245 0.25  0.255 0.26  0.265 0.27  0.275 0.28  0.285 0.29  0.295\n",
      " 0.3   0.305 0.31  0.315 0.32  0.325 0.33  0.335 0.34  0.345 0.35  0.355\n",
      " 0.36  0.365 0.37  0.375 0.38  0.385 0.39  0.395 0.4   0.405 0.41  0.415\n",
      " 0.42  0.425 0.43  0.435 0.44  0.445 0.45  0.455 0.46  0.465 0.47  0.475\n",
      " 0.48  0.485 0.49  0.495 0.5   0.505 0.51  0.515 0.52  0.525 0.53  0.535\n",
      " 0.54  0.545 0.55  0.555 0.56  0.565 0.57  0.575 0.58  0.585 0.59  0.595\n",
      " 0.6   0.605 0.61  0.615 0.62  0.625 0.63  0.635 0.64  0.645 0.65  0.655\n",
      " 0.66  0.665 0.67  0.675 0.68  0.685 0.69  0.695 0.7   0.705 0.71  0.715\n",
      " 0.72  0.725 0.73  0.735 0.74  0.745 0.75  0.755 0.76  0.765 0.77  0.775\n",
      " 0.78  0.785 0.79  0.795 0.8   0.805 0.81  0.815 0.82  0.825 0.83  0.835\n",
      " 0.84  0.845 0.85  0.855 0.86  0.865 0.87  0.875 0.88  0.885 0.89  0.895\n",
      " 0.9   0.905 0.91  0.915 0.92  0.925 0.93  0.935 0.94  0.945 0.95  0.955\n",
      " 0.96  0.965 0.97  0.975 0.98  0.985 0.99  0.995 1.   ]\n"
     ]
    }
   ],
   "source": [
    "data2 = np.loadtxt(outputs / 'data.dat')\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.74323717]]\n",
      "[[0.05263158 0.47339885]]\n",
      "[[0.10526316 0.59716101]]\n",
      "[[0.15789474 0.61335066]]\n",
      "[[0.21052632 0.11946593]]\n",
      "[[0.26315789 0.90633945]]\n",
      "[[0.31578947 0.11381917]]\n",
      "[[0.36842105 0.60659634]]\n",
      "[[0.42105263 0.45403256]]\n",
      "[[0.47368421 0.91076152]]\n",
      "[[0.52631579 0.52970948]]\n",
      "[[0.57894737 0.0088362 ]]\n",
      "[[0.63157895 0.22975936]]\n",
      "[[0.68421053 0.09459466]]\n",
      "[[0.73684211 0.19553725]]\n",
      "[[0.78947368 0.06313101]]\n",
      "[[0.84210526 0.97673542]]\n",
      "[[0.89473684 0.11682906]]\n",
      "[[0.94736842 0.61214968]]\n",
      "[[1.         0.48426655]]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 1, 20)\n",
    "y = np.random.random(20)\n",
    "header = \"X-Column, Y-Column\\n\"\n",
    "header += \"This is a second line\"\n",
    "\n",
    "with open(outputs / 'data2.dat', 'wb') as f:\n",
    "#with open(outputs / 'data2.dat', 'w') as f:   #using text formatting and no binary files\n",
    "    np.savetxt(f, [], header=header)\n",
    "    for i in range(20):\n",
    "        data = np.column_stack((x[i], y[i]))\n",
    "        np.savetxt(f, data), print(data)\n",
    "        #f.write('{:4.1f} {:.4f}\\n'.format(x[i], y[i]))    #using text formatting and no binary files\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# X-Column, Y-Column\n",
      "# This is a second line\n",
      "0.000000000000000000e+00 7.432371698109303537e-01\n",
      "5.263157894736841813e-02 4.733988504818925636e-01\n",
      "1.052631578947368363e-01 5.971610138538376855e-01\n",
      "1.578947368421052544e-01 6.133506611533304786e-01\n",
      "2.105263157894736725e-01 1.194659336964386531e-01\n",
      "2.631578947368420907e-01 9.063394468535985737e-01\n",
      "3.157894736842105088e-01 1.138191666344308128e-01\n",
      "3.684210526315789269e-01 6.065963372716890456e-01\n",
      "4.210526315789473450e-01 4.540325551247034186e-01\n",
      "4.736842105263157632e-01 9.107615248601377278e-01\n",
      "5.263157894736841813e-01 5.297094763486214219e-01\n",
      "5.789473684210526550e-01 8.836197853132721214e-03\n",
      "6.315789473684210176e-01 2.297593606184347159e-01\n",
      "6.842105263157893802e-01 9.459466170426644727e-02\n",
      "7.368421052631578538e-01 1.955372460217442709e-01\n",
      "7.894736842105263275e-01 6.313101046853319964e-02\n",
      "8.421052631578946901e-01 9.767354237093872626e-01\n",
      "8.947368421052630527e-01 1.168290620874751218e-01\n",
      "9.473684210526315264e-01 6.121496755126265565e-01\n",
      "1.000000000000000000e+00 4.842665527128495739e-01\n"
     ]
    }
   ],
   "source": [
    "! cat ../outputs/data2.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = [1, 1.2, 'a', 'b']\n",
    "\n",
    "with open(outputs / 'data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.2, 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "with open(outputs / 'data.pkl', 'rb') as f:\n",
    "    new_data = pickle.load(f)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = \"beatles-diskography.csv\"\n",
    "datafile = datapath / csvfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Please Please Me',\n",
       "  'Released': '22 March 1963',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '-',\n",
       "  'BPI Certification': 'Gold',\n",
       "  'RIAA Certification': 'Platinum'},\n",
       " {'Title': 'With the Beatles',\n",
       "  'Released': '22 November 1963',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '-',\n",
       "  'BPI Certification': 'Platinum',\n",
       "  'RIAA Certification': 'Gold'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "#with open(datafile, \"r\") as f:\n",
    "with open(os.path.join(\"../datasets/CSVs/\", csvfile), \"r\") as f:\n",
    "    keys = f.readline().split(',')\n",
    "    keys = [k.strip() for k in keys]\n",
    "    for i in range(5):\n",
    "        values = f.readline().split(',')\n",
    "        values = [v.strip() for v in values]\n",
    "        d = dict(zip(keys,values))\n",
    "        data.append(d)\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using [CSV](https://docs.python.org/3/library/csv.html) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Title', 'Please Please Me'), ('Released', '22 March 1963'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'With the Beatles'), ('Released', '22 November 1963'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', 'Gold')])\n",
      "OrderedDict([('Title', 'Beatlemania! With the Beatles'), ('Released', '25 November 1963'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Introducing... The Beatles'), ('Released', '10 January 1964'), ('Label', 'Vee-Jay(US)'), ('UK Chart Position', '-'), ('US Chart Position', '2'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Meet the Beatles!'), ('Released', '20 January 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '5xPlatinum')])\n",
      "OrderedDict([('Title', 'Twist and Shout'), ('Released', '3 February 1964'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', \"The Beatles' Second Album\"), ('Released', '10 April 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '2xPlatinum')])\n",
      "OrderedDict([('Title', \"The Beatles' Long Tall Sally\"), ('Released', '11 May 1964'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', \"A Hard Day's Night\"), ('Released', '26 June 1964'), ('Label', 'United Artists(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '4xPlatinum')])\n",
      "OrderedDict([('Title', ''), ('Released', '10 July 1964'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Something New'), ('Released', '20 July 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '2'), ('BPI Certification', ''), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Beatles for Sale'), ('Released', '4 December 1964'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', \"Beatles '65\"), ('Released', '15 December 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '3xPlatinum')])\n",
      "OrderedDict([('Title', 'Beatles VI'), ('Released', '14 June 1965'), ('Label', 'Parlophone(NZ), Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Help!'), ('Released', '6 August 1965'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '13 August 1965'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '3xPlatinum')])\n",
      "OrderedDict([('Title', 'Rubber Soul'), ('Released', '3 December 1965'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '6 December 1965'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '6xPlatinum')])\n",
      "OrderedDict([('Title', 'Yesterday and Today'), ('Released', '15 June 1966'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '2xPlatinum')])\n",
      "OrderedDict([('Title', 'Revolver'), ('Released', '5 August 1966'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '8 August 1966'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '5xPlatinum')])\n",
      "OrderedDict([('Title', \"Sgt. Pepper's Lonely Hearts Club Band\"), ('Released', '1 June 1967'), ('Label', 'Parlophone(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', '3xPlatinum'), ('RIAA Certification', '11xPlatinum')])\n",
      "OrderedDict([('Title', 'Magical Mystery Tour'), ('Released', '27 November 1967'), ('Label', 'Parlophone(UK), Capitol(US)'), ('UK Chart Position', '31[D]'), ('US Chart Position', '1'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '6xPlatinum')])\n",
      "OrderedDict([('Title', 'The Beatles'), ('Released', '22 November 1968'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '19xPlatinum')])\n",
      "OrderedDict([('Title', 'Yellow Submarine'), ('Released', '13 January 1969'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '3'), ('US Chart Position', '2'), ('BPI Certification', 'Silver'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Abbey Road'), ('Released', '26 September 1969'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', '2xPlatinum'), ('RIAA Certification', '12xPlatinum')])\n",
      "OrderedDict([('Title', 'Let It Be'), ('Released', '8 May 1970'), ('Label', 'Apple(UK),United Artists(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', 'Gold'), ('RIAA Certification', '4xPlatinum')])\n"
     ]
    }
   ],
   "source": [
    "#with open(datafile, \"r\") as f:\n",
    "with open(os.path.join(\"../datasets/CSVs/\", csvfile), \"r\") as f:\n",
    "    #data2 = csv.reader(f)\n",
    "    data2 = csv.DictReader(f)\n",
    "    for row in data2:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Released</th>\n",
       "      <th>Label</th>\n",
       "      <th>UK Chart Position</th>\n",
       "      <th>US Chart Position</th>\n",
       "      <th>BPI Certification</th>\n",
       "      <th>RIAA Certification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please Please Me</td>\n",
       "      <td>22 March 1963</td>\n",
       "      <td>Parlophone(UK)</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Platinum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the Beatles</td>\n",
       "      <td>22 November 1963</td>\n",
       "      <td>Parlophone(UK)</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>Gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beatlemania! With the Beatles</td>\n",
       "      <td>25 November 1963</td>\n",
       "      <td>Capitol(CAN)</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Introducing... The Beatles</td>\n",
       "      <td>10 January 1964</td>\n",
       "      <td>Vee-Jay(US)</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meet the Beatles!</td>\n",
       "      <td>20 January 1964</td>\n",
       "      <td>Capitol(US)</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5xPlatinum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title          Released           Label  \\\n",
       "0               Please Please Me     22 March 1963  Parlophone(UK)   \n",
       "1               With the Beatles  22 November 1963  Parlophone(UK)   \n",
       "2  Beatlemania! With the Beatles  25 November 1963    Capitol(CAN)   \n",
       "3     Introducing... The Beatles   10 January 1964     Vee-Jay(US)   \n",
       "4              Meet the Beatles!   20 January 1964     Capitol(US)   \n",
       "\n",
       "  UK Chart Position US Chart Position BPI Certification RIAA Certification  \n",
       "0                 1                 -              Gold           Platinum  \n",
       "1                 1                 -          Platinum               Gold  \n",
       "2                 -                 -               NaN                NaN  \n",
       "3                 -                 2               NaN                NaN  \n",
       "4                 -                 1               NaN         5xPlatinum  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = pd.read_csv(datafile)\n",
    "df_csv.head()\n",
    "#df_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating [Zip Files](https://docs.python.org/3/library/zipfile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressedfile = \"2013_ERCOT_Hourly_Load_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour_End</th>\n",
       "      <th>COAST</th>\n",
       "      <th>EAST</th>\n",
       "      <th>FAR_WEST</th>\n",
       "      <th>NORTH</th>\n",
       "      <th>NORTH_C</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>SOUTH_C</th>\n",
       "      <th>WEST</th>\n",
       "      <th>ERCOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>7606.263544</td>\n",
       "      <td>1073.892438</td>\n",
       "      <td>1411.750567</td>\n",
       "      <td>784.978166</td>\n",
       "      <td>10369.094390</td>\n",
       "      <td>2206.675077</td>\n",
       "      <td>4368.490945</td>\n",
       "      <td>882.931901</td>\n",
       "      <td>28704.077028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>7388.082714</td>\n",
       "      <td>1035.021938</td>\n",
       "      <td>1403.472287</td>\n",
       "      <td>776.307387</td>\n",
       "      <td>10152.358518</td>\n",
       "      <td>2159.733208</td>\n",
       "      <td>4233.587967</td>\n",
       "      <td>872.404750</td>\n",
       "      <td>28020.968769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>7178.867878</td>\n",
       "      <td>1036.088697</td>\n",
       "      <td>1395.053150</td>\n",
       "      <td>768.125748</td>\n",
       "      <td>9988.051418</td>\n",
       "      <td>2065.114706</td>\n",
       "      <td>4082.862860</td>\n",
       "      <td>868.853938</td>\n",
       "      <td>27383.018395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>7038.822581</td>\n",
       "      <td>1032.648841</td>\n",
       "      <td>1395.508820</td>\n",
       "      <td>770.937969</td>\n",
       "      <td>9946.658655</td>\n",
       "      <td>1990.903699</td>\n",
       "      <td>4010.489608</td>\n",
       "      <td>865.701201</td>\n",
       "      <td>27051.671374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>6990.857940</td>\n",
       "      <td>1042.823044</td>\n",
       "      <td>1401.216842</td>\n",
       "      <td>779.089313</td>\n",
       "      <td>10096.664190</td>\n",
       "      <td>1954.807585</td>\n",
       "      <td>4038.655997</td>\n",
       "      <td>879.924249</td>\n",
       "      <td>27184.039160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hour_End        COAST         EAST     FAR_WEST       NORTH  \\\n",
       "0 2013-01-01 01:00:00  7606.263544  1073.892438  1411.750567  784.978166   \n",
       "1 2013-01-01 02:00:00  7388.082714  1035.021938  1403.472287  776.307387   \n",
       "2 2013-01-01 03:00:00  7178.867878  1036.088697  1395.053150  768.125748   \n",
       "3 2013-01-01 04:00:00  7038.822581  1032.648841  1395.508820  770.937969   \n",
       "4 2013-01-01 05:00:00  6990.857940  1042.823044  1401.216842  779.089313   \n",
       "\n",
       "        NORTH_C     SOUTHERN      SOUTH_C        WEST         ERCOT  \n",
       "0  10369.094390  2206.675077  4368.490945  882.931901  28704.077028  \n",
       "1  10152.358518  2159.733208  4233.587967  872.404750  28020.968769  \n",
       "2   9988.051418  2065.114706  4082.862860  868.853938  27383.018395  \n",
       "3   9946.658655  1990.903699  4010.489608  865.701201  27051.671374  \n",
       "4  10096.664190  1954.807585  4038.655997  879.924249  27184.039160  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with ZipFile('{0}.zip'.format(datapath / compressedfile), 'r') as myzip:\n",
    "    #with myzip.open('2013_ERCOT_Hourly_Load_Data.xls') as myfile:\n",
    "    #    pd_excel = pd.read_excel(myfile)\n",
    "    myzip.extract('{0}.xls'.format(compressedfile))\n",
    "                  \n",
    "pd_excel = pd.read_excel('{0}.xls'.format(datapath / compressedfile))\n",
    "pd_excel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Acessing](https://pypi.python.org/pypi/xlrd) and [Writing](https://pypi.python.org/pypi/xlwt) [Excel Files](https://github.com/python-excel/tutorial/blob/master/python-excel.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip:\n",
    "    myzip.extractall(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlrd.open_workbook(datapath / datafile)\n",
    "sheet = workbook.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hour_End',\n",
       "  'COAST',\n",
       "  'EAST',\n",
       "  'FAR_WEST',\n",
       "  'NORTH',\n",
       "  'NORTH_C',\n",
       "  'SOUTHERN',\n",
       "  'SOUTH_C',\n",
       "  'WEST',\n",
       "  'ERCOT'],\n",
       " [41275.041666666664,\n",
       "  7606.263544000012,\n",
       "  1073.892438,\n",
       "  1411.7505669999982,\n",
       "  784.9781659999992,\n",
       "  10369.094390000051,\n",
       "  2206.6750770000012,\n",
       "  4368.490945000006,\n",
       "  882.9319009999975,\n",
       "  28704.077028000065]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "sheet_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sheet:\n",
      "7296\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the sheet:\"),\n",
    "print(sheet.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data in cell (row 3, col 2):\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of data in cell (row 3, col 2):\"), \n",
    "print(sheet.cell_type(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value in cell (row 3, col 2):\n",
      "1036.0886969999988\n"
     ]
    }
   ],
   "source": [
    "print(\"Value in cell (row 3, col 2):\", )\n",
    "print(sheet.cell_value(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n"
     ]
    }
   ],
   "source": [
    "print(\"Get a slice of values in column 3, from rows 1-3:\",)\n",
    "print(sheet.col_values(3, start_rowx=1, end_rowx=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "coast = sheet.col_values(1, start_rowx=1)\n",
    "data3 = {}    \n",
    "data3['maxvalue'] = max(coast)\n",
    "data3['minvalue'] = min(coast)\n",
    "data3['avgcoast'] = np.mean(coast)\n",
    "\n",
    "rowmax = coast.index(max(coast))+1\n",
    "rowmin = coast.index(min(coast))+1\n",
    "\n",
    "data3['maxtime'] = xlrd.xldate_as_tuple(sheet.cell_value(rowmax,0), 0)\n",
    "data3['mintime'] = xlrd.xldate_as_tuple(sheet.cell_value(rowmin,0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maxvalue': 18779.025510000003,\n",
       " 'minvalue': 6602.113898999982,\n",
       " 'avgcoast': 10976.933460679784,\n",
       " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
       " 'mintime': (2013, 2, 3, 4, 0, 0)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour_End</th>\n",
       "      <th>COAST</th>\n",
       "      <th>EAST</th>\n",
       "      <th>FAR_WEST</th>\n",
       "      <th>NORTH</th>\n",
       "      <th>NORTH_C</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>SOUTH_C</th>\n",
       "      <th>WEST</th>\n",
       "      <th>ERCOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>7606.263544</td>\n",
       "      <td>1073.892438</td>\n",
       "      <td>1411.750567</td>\n",
       "      <td>784.978166</td>\n",
       "      <td>10369.094390</td>\n",
       "      <td>2206.675077</td>\n",
       "      <td>4368.490945</td>\n",
       "      <td>882.931901</td>\n",
       "      <td>28704.077028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>7388.082714</td>\n",
       "      <td>1035.021938</td>\n",
       "      <td>1403.472287</td>\n",
       "      <td>776.307387</td>\n",
       "      <td>10152.358518</td>\n",
       "      <td>2159.733208</td>\n",
       "      <td>4233.587967</td>\n",
       "      <td>872.404750</td>\n",
       "      <td>28020.968769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>7178.867878</td>\n",
       "      <td>1036.088697</td>\n",
       "      <td>1395.053150</td>\n",
       "      <td>768.125748</td>\n",
       "      <td>9988.051418</td>\n",
       "      <td>2065.114706</td>\n",
       "      <td>4082.862860</td>\n",
       "      <td>868.853938</td>\n",
       "      <td>27383.018395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>7038.822581</td>\n",
       "      <td>1032.648841</td>\n",
       "      <td>1395.508820</td>\n",
       "      <td>770.937969</td>\n",
       "      <td>9946.658655</td>\n",
       "      <td>1990.903699</td>\n",
       "      <td>4010.489608</td>\n",
       "      <td>865.701201</td>\n",
       "      <td>27051.671374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>6990.857940</td>\n",
       "      <td>1042.823044</td>\n",
       "      <td>1401.216842</td>\n",
       "      <td>779.089313</td>\n",
       "      <td>10096.664190</td>\n",
       "      <td>1954.807585</td>\n",
       "      <td>4038.655997</td>\n",
       "      <td>879.924249</td>\n",
       "      <td>27184.039160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hour_End        COAST         EAST     FAR_WEST       NORTH  \\\n",
       "0 2013-01-01 01:00:00  7606.263544  1073.892438  1411.750567  784.978166   \n",
       "1 2013-01-01 02:00:00  7388.082714  1035.021938  1403.472287  776.307387   \n",
       "2 2013-01-01 03:00:00  7178.867878  1036.088697  1395.053150  768.125748   \n",
       "3 2013-01-01 04:00:00  7038.822581  1032.648841  1395.508820  770.937969   \n",
       "4 2013-01-01 05:00:00  6990.857940  1042.823044  1401.216842  779.089313   \n",
       "\n",
       "        NORTH_C     SOUTHERN      SOUTH_C        WEST         ERCOT  \n",
       "0  10369.094390  2206.675077  4368.490945  882.931901  28704.077028  \n",
       "1  10152.358518  2159.733208  4233.587967  872.404750  28020.968769  \n",
       "2   9988.051418  2065.114706  4082.862860  868.853938  27383.018395  \n",
       "3   9946.658655  1990.903699  4010.489608  865.701201  27051.671374  \n",
       "4  10096.664190  1954.807585  4038.655997  879.924249  27184.039160  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_excel = pd.read_excel(datapath / datafile)\n",
    "pd_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Writing a Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "style0 = xlwt.easyxf('font: name Times New Roman, color-index red, bold on', num_format_str='#,##0.00')\n",
    "style1 = xlwt.easyxf(num_format_str='D-MMM-YY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlwt.Workbook()\n",
    "workbook_name = 'my_workbook.xls'\n",
    "\n",
    "worksheet1 = workbook.add_sheet('my_first_sheet')\n",
    "worksheet1.write(0, 0, 'Column 1 title', style0)\n",
    "worksheet1.write(0, 1, 'Column 2 title', style0)\n",
    "\n",
    "worksheet2 = workbook.add_sheet('my_second_sheet')\n",
    "worksheet2.write(0, 0, 'Itens')\n",
    "worksheet2.write(0, 1, 'Price')\n",
    "worksheet2.write(1, 0, 'Rice')\n",
    "worksheet2.write(1, 1, 100)\n",
    "worksheet2.write(2, 0, 'Beans')\n",
    "worksheet2.write(2, 1, 200)\n",
    "worksheet2.write(3, 0, 'Pasta')\n",
    "worksheet2.write(3, 1, 500)\n",
    "worksheet2.write(5, 0, 'Total')\n",
    "\n",
    "worksheet2.write(5, 1, xlwt.Formula(\"B2+B3+B4\"))\n",
    "\n",
    "\n",
    "workbook.save(outputs / workbook_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading PDF Files  \n",
    "\n",
    "More resources [here](https://textract.readthedocs.io/en/stable/) and [here](https://towardsdatascience.com/python-for-pdf-ef0fac2808b0?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the folder:\n",
      "1 -- Useful_Things_ML.pdf\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in os.listdir(datapath2) if os.path.isfile(datapath2 / f) and f.endswith('.pdf')]\n",
    "onlyfiles.sort()\n",
    "\n",
    "print('Files in the folder:')\n",
    "for i, w in enumerate(onlyfiles):\n",
    "    print(i+1, '--' ,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Few Useful Things to Know about Machine Learning\n",
      "Pedro Domingos\n",
      "Department of Computer Science and Engineering\n",
      "University of Washington\n",
      "Seattle, WA 98195-2350, U.S.A.\n",
      "\n",
      "pedrod@cs.washington.edu\n",
      "\n",
      "ABSTRACT\n",
      "Machine learning algorithms can figure out how to perform\n",
      "important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming\n",
      "is not. As more data becomes available, more ambitious\n",
      "problems can be tackled. As a result, machine learning is\n",
      "widely used in computer science and other fields. However,\n",
      "developing successful machine learning applications requires\n",
      "a substantial amount of “black art” that is hard to find in\n",
      "textbooks. This article summarizes twelve key lessons that\n",
      "machine learning researchers and practitioners have learned.\n",
      "These include pitfalls to avoid, important issues to focus on,\n",
      "and answers to common questions.\n",
      "\n",
      "correct output yt for future examples xt (e.g., whether the\n",
      "spam filter correctly classifies previously unseen em\n"
     ]
    }
   ],
   "source": [
    "text0 = textract.process(datapath2 / onlyfiles[0]).decode('utf-8')\n",
    "print(text0[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text in Image Files (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the folder:\n",
      "1 -- Cov_nonstationary.png\n",
      "2 -- K-means.png\n",
      "3 -- Mean_nonstationary.png\n",
      "4 -- Modeling1.png\n",
      "5 -- Modeling2.png\n",
      "6 -- Modeling4.png\n",
      "7 -- Var_nonstationary.png\n",
      "8 -- aristotle-wikipedia.png\n",
      "9 -- binning.png\n",
      "10 -- db_schema.png\n",
      "11 -- decision_tree.png\n",
      "12 -- dimreduxsummary.png\n",
      "13 -- entities1.png\n",
      "14 -- entities2.png\n",
      "15 -- framework.png\n",
      "16 -- groupby-example.png\n",
      "17 -- hclustering-1.png\n",
      "18 -- hclustering-2.png\n",
      "19 -- lenet_architecture-768x226.png\n",
      "20 -- machine-learning-cheet-sheet.png\n",
      "21 -- modeling3.png\n",
      "22 -- neuralnet.png\n",
      "23 -- neuralnet2.png\n",
      "24 -- neuralnet3.png\n",
      "25 -- neuralnet4.png\n",
      "26 -- nmf.png\n",
      "27 -- pca_ica.png\n",
      "28 -- pca_ica2.png\n",
      "29 -- pca_ica3.png\n",
      "30 -- project1.png\n",
      "31 -- project2.png\n",
      "32 -- project3.png\n",
      "33 -- project4.png\n",
      "34 -- sample_graph.png\n",
      "35 -- saturn.png\n",
      "36 -- straight_line.png\n",
      "37 -- summary_redux_dim_techniques.png\n",
      "38 -- sup-unsup.png\n",
      "39 -- textimage.png\n",
      "40 -- transform-example.png\n",
      "41 -- transmission.png\n",
      "42 -- tsne1.png\n",
      "43 -- tsne2.png\n",
      "44 -- vehicle_class.png\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in os.listdir(datapath3) if os.path.isfile(datapath3 / f) and f.endswith('.png')]\n",
    "onlyfiles.sort()\n",
    "\n",
    "print('Files in the folder:')\n",
    "for i, w in enumerate(onlyfiles):\n",
    "    print(i+1, '--' ,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../datasets/Figs/textimage.png')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath3 / 'textimage.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adobe, the Adobe logo, Acrobat, the Acrobat logo, Acrobat Capture, Adobe Garamond, Adobe\n",
      "Intelligent Document Platform, Adobe PDF, Adobe Reader, Adobe Solutions Network, Aldus, Dis-\n",
      "tiller, ePaper, Extreme, FrameMaker, Illustrator, InDesign, Minion, Myriad, PageMaker, Photo-\n",
      "shop, Poetica, PostScript, and XMP are either registered trademarks or trademarks of Adobe\n",
      "‘Systems Incorporated in the United States and/or other countries. Microsoft and Windows are\n",
      "either registered trademarks or trademarks of Microsoft Corporation in the United States and/or\n",
      "other countries. Apple, Mac, Macintosh, and Power Macintosh are trademarks of Apple Computer,\n",
      "Inc,, registered in the United States and other countries. IBM is a registered trademark of IBM\n",
      "Corporation in the United States. Sun is a trademark or registered trademark of Sun Microsys-\n",
      "tems, Inc. in the United States and other countries. UNIX is\n"
     ]
    }
   ],
   "source": [
    "text1 = textract.process(datapath3 / 'textimage.png').decode('utf-8')\n",
    "print(text1[0:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/urllib.html  \n",
    "http://www.crummy.com/software/BeautifulSoup/  \n",
    "http://docs.python-requests.org/en/latest/  \n",
    "http://lxml.de/  \n",
    "https://docs.python.org/3/library/getpass.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import lxml.html\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oglobo.globo.com/',\n",
       " '#main-nav',\n",
       " 'https://www.facebook.com/jornaloglobo',\n",
       " 'https://twitter.com/jornaloglobo',\n",
       " 'https://www.instagram.com/jornaloglobo',\n",
       " 'https://oglobo.globo.com/rio/',\n",
       " 'https://oglobo.globo.com/brasil/',\n",
       " 'https://oglobo.globo.com/mundo/',\n",
       " 'https://oglobo.globo.com/economia/',\n",
       " 'https://oglobo.globo.com/cultura/']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://oglobo.globo.com\"\n",
    "html_txt = urllib.request.urlopen(url).read()\n",
    "soup = bs(html_txt, \"html.parser\")\n",
    " \n",
    "[line.get('href') for line in soup.find_all('a')][0:10]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach: [lxml](http://lxml.de/tutorial.html) instead of beautiful soup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.hotashtanga.com/',\n",
       " 'http://www.hotashtanga.com/',\n",
       " 'http://www.hotashtanga.com/p/szolgaltatasok.html',\n",
       " 'http://www.hotashtanga.com/p/letoltesek-downloads.html',\n",
       " 'http://www.hotashtanga.com/p/eletrajz.html',\n",
       " 'http://www.hotashtanga.com/p/kapcsolat-contact-me.html',\n",
       " '//www.blogger.com/rearrange?blogID=2389334795161374820&widgetType=PageList&widgetId=PageList1&action=editWidget&sectionId=crosscol',\n",
       " 'https://drive.google.com/open?id=0B0t9QIGAtRQ5eGVjNDFaQmFhZXM',\n",
       " 'https://drive.google.com/open?id=0B0t9QIGAtRQ5dnJEamZaWmpieGs',\n",
       " 'https://drive.google.com/open?id=0B0t9QIGAtRQ5eGMzX0U4ZVFNNjA']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.hotashtanga.com/p/letoltesek-downloads.html\"\n",
    "html_txt = urllib.request.urlopen(url).read()\n",
    "dom =  lxml.html.fromstring(html_txt)\n",
    "    \n",
    "[line for line in dom.xpath('//a/@href')][0:10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using [Requests](http://docs.python-requests.org/en/master/user/quickstart/), for more complex tasks  \n",
    "\n",
    "\n",
    "c.f. [HTML Responses](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ············\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = getpass.getpass()\n",
    "r = requests.get('https://api.github.com/user', auth=('rsouza', p))\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/json; charset=utf-8'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"login\":\"rsouza\",\"id\":604914,\"node_id\":\"MDQ6VXNlcjYwNDkxNA==\",\"avatar_url\":\"https://avatars0.githubusercontent.com/u/604914?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/rsouza\",\"html_url\":\"https://github.com/rsouza\",\"followers_url\":\"https://api.github.com/users/rsouza/followers\",\"following_url\":\"https://api.github.com/users/rsouza/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/rsouza/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/rsouza/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/rsouza/subscriptions\",\"organizations_url\":\"https://api.github.com/users/rsouza/orgs\",\"repos_url\":\"https://api.github.com/users/rsouza/repos\",\"events_url\":\"https://api.github.com/users/rsouza/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/rsouza/received_events\",\"type\":\"User\",\"site_admin\":false,\"name\":\"Renato Rocha Souza\",\"company\":\"ÖAW-ACDH | FGV | UFMG\",\"blog\":\"https://www.oeaw.ac.at/acdh/team/current-team/renato-souza/\",\"location\":\"Viena\",\"email\":\"rsouza.fgv@gmail.com\",\"hireable\":null,\"bio\":\"https://www.linkedin.com/in/renato-rocha-souza-157153\",\"public_repos\":35,\"public_gists\":2,\"followers\":97,\"following\":9,\"created_at\":\"2011-02-07T15:46:45Z\",\"updated_at\":\"2019-11-24T20:10:16Z\",\"private_gists\":0,\"total_private_repos\":1,\"owned_private_repos\":0,\"disk_usage\":1386898,\"collaborators\":0,\"two_factor_authentication\":false,\"plan\":{\"name\":\"pro\",\"space\":976562499,\"collaborators\":0,\"private_repos\":9999}}'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'login': 'rsouza',\n",
       " 'id': 604914,\n",
       " 'node_id': 'MDQ6VXNlcjYwNDkxNA==',\n",
       " 'avatar_url': 'https://avatars0.githubusercontent.com/u/604914?v=4',\n",
       " 'gravatar_id': '',\n",
       " 'url': 'https://api.github.com/users/rsouza',\n",
       " 'html_url': 'https://github.com/rsouza',\n",
       " 'followers_url': 'https://api.github.com/users/rsouza/followers',\n",
       " 'following_url': 'https://api.github.com/users/rsouza/following{/other_user}',\n",
       " 'gists_url': 'https://api.github.com/users/rsouza/gists{/gist_id}',\n",
       " 'starred_url': 'https://api.github.com/users/rsouza/starred{/owner}{/repo}',\n",
       " 'subscriptions_url': 'https://api.github.com/users/rsouza/subscriptions',\n",
       " 'organizations_url': 'https://api.github.com/users/rsouza/orgs',\n",
       " 'repos_url': 'https://api.github.com/users/rsouza/repos',\n",
       " 'events_url': 'https://api.github.com/users/rsouza/events{/privacy}',\n",
       " 'received_events_url': 'https://api.github.com/users/rsouza/received_events',\n",
       " 'type': 'User',\n",
       " 'site_admin': False,\n",
       " 'name': 'Renato Rocha Souza',\n",
       " 'company': 'ÖAW-ACDH | FGV | UFMG',\n",
       " 'blog': 'https://www.oeaw.ac.at/acdh/team/current-team/renato-souza/',\n",
       " 'location': 'Viena',\n",
       " 'email': 'rsouza.fgv@gmail.com',\n",
       " 'hireable': None,\n",
       " 'bio': 'https://www.linkedin.com/in/renato-rocha-souza-157153',\n",
       " 'public_repos': 35,\n",
       " 'public_gists': 2,\n",
       " 'followers': 97,\n",
       " 'following': 9,\n",
       " 'created_at': '2011-02-07T15:46:45Z',\n",
       " 'updated_at': '2019-11-24T20:10:16Z',\n",
       " 'private_gists': 0,\n",
       " 'total_private_repos': 1,\n",
       " 'owned_private_repos': 0,\n",
       " 'disk_usage': 1386898,\n",
       " 'collaborators': 0,\n",
       " 'two_factor_authentication': False,\n",
       " 'plan': {'name': 'pro',\n",
       "  'space': 976562499,\n",
       "  'collaborators': 0,\n",
       "  'private_repos': 9999}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing [JSON](http://json.org/)\n",
    "\n",
    "https://docs.python.org/3/library/json.html  \n",
    "https://docs.python.org/3/library/urllib.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://musicbrainz.org/ws/2/\"\n",
    "artist_url =  urllib.parse.urljoin(base_url, 'artist/')\n",
    "query_type = {\"simple\": {},\n",
    "              \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "              \"aliases\": {\"inc\": \"aliases\"},\n",
    "              \"releases\": {\"inc\": \"releases\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print(\"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_name(url, params, name):\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data, indent=4):\n",
    "    if type(data) == dict:\n",
    "        print(json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "[{'id': '5b11f4ce-a62d-471e-81fc-a69a8278c7da', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 100, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'country': 'US', 'area': {'id': '489ce91b-6658-3307-9877-795b68554c98', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'United States', 'sort-name': 'United States', 'life-span': {'ended': None}}, 'begin-area': {'id': 'a640b45c-c173-49b1-8030-973603e895b5', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Aberdeen', 'sort-name': 'Aberdeen', 'life-span': {'ended': None}}, 'disambiguation': '90s US grunge band', 'isnis': ['0000000123486830', '0000000123487390'], 'life-span': {'begin': '1988-01', 'end': '1994-04-05', 'ended': True}, 'aliases': [{'sort-name': 'ニルヴァーナ', 'type-id': '894afba6-2816-3c24-8072-eadb66bd04bc', 'name': 'ニルヴァーナ', 'locale': 'ja', 'type': 'Artist name', 'primary': True, 'begin-date': None, 'end-date': None}, {'sort-name': 'Nirvana US', 'name': 'Nirvana US', 'locale': None, 'type': None, 'primary': None, 'begin-date': None, 'end-date': None}], 'tags': [{'count': 15, 'name': 'rock'}, {'count': 9, 'name': 'alternative rock'}, {'count': 1, 'name': '90s'}, {'count': 3, 'name': 'punk'}, {'count': 7, 'name': 'american'}, {'count': 2, 'name': 'experimental'}, {'count': 2, 'name': 'seattle'}, {'count': 22, 'name': 'grunge'}, {'count': 0, 'name': 'band'}, {'count': 1, 'name': 'usa'}, {'count': 0, 'name': 'alternative'}, {'count': 0, 'name': 'américain'}, {'count': 0, 'name': 'legendary'}, {'count': 1, 'name': 'acoustic rock'}, {'count': 3, 'name': 'noise rock'}, {'count': 0, 'name': '90'}, {'count': 0, 'name': 'northwest'}, {'count': 0, 'name': 'rock and indie'}, {'count': 0, 'name': 'united states'}, {'count': 0, 'name': 'nirvana'}, {'count': 0, 'name': 'kurt cobain'}]}, {'id': '9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 79, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'country': 'GB', 'area': {'id': '8a754a16-0027-3a29-b6d7-2b40ea0481ed', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'United Kingdom', 'sort-name': 'United Kingdom', 'life-span': {'ended': None}}, 'begin-area': {'id': 'f03d09b3-39dc-4083-afd6-159e3f0d462f', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'London', 'sort-name': 'London', 'life-span': {'ended': None}}, 'disambiguation': '60s band from the UK', 'life-span': {'begin': '1967', 'ended': None}, 'tags': [{'count': 1, 'name': 'psychedelic rock'}, {'count': 1, 'name': 'pop'}, {'count': 1, 'name': 'british'}, {'count': 1, 'name': 'psychedelic pop'}, {'count': 0, 'name': 'orchestral'}, {'count': 1, 'name': 'pop rock'}, {'count': 2, 'name': 'rock'}, {'count': 1, 'name': 'soft rock'}, {'count': 1, 'name': 'english'}, {'count': 2, 'name': 'progressive rock'}, {'count': 1, 'name': 'baroque pop'}, {'count': 1, 'name': 'symphonic rock'}, {'count': 1, 'name': 'power pop'}]}, {'id': 'c3a64a25-251b-4d03-afba-1471440245b8', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 74, 'name': 'Approaching Nirvana', 'sort-name': 'Approaching Nirvana', 'country': 'US', 'area': {'id': '489ce91b-6658-3307-9877-795b68554c98', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'United States', 'sort-name': 'United States', 'life-span': {'ended': None}}, 'life-span': {'begin': '2009', 'ended': None}}, {'id': 'c49d69dc-e008-47cf-b5ff-160fafb1fe1f', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 71, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'country': 'FR', 'area': {'id': '08310658-51eb-3801-80de-5a0739207115', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'France', 'sort-name': 'France', 'life-span': {'ended': None}}, 'disambiguation': '’70s French band from Martigues', 'life-span': {'ended': None}}, {'id': '85af0709-95db-4fbc-801a-120e9f4766d0', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 71, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'country': 'FI', 'area': {'id': '6a264f94-6ff1-30b1-9a81-41f7bfabd616', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'Finland', 'sort-name': 'Finland', 'life-span': {'ended': None}}, 'disambiguation': \"Early 1980's Finnish punk band\", 'life-span': {'ended': None}, 'tags': [{'count': 1, 'name': 'punk'}, {'count': 1, 'name': 'finland'}]}, {'id': '28a4618c-38e4-4027-ad2c-db66a14a2d85', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 71, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'country': 'YU', 'area': {'id': '885dce63-c211-3033-8cf7-46cb82d440c7', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'Yugoslavia', 'sort-name': 'Yugoslavia', 'life-span': {'begin': '1918', 'end': '2003', 'ended': True}}, 'disambiguation': 'Croatian prog-rock band active in first half of 70s in former Yugoslavia.', 'life-span': {'ended': None}}, {'id': '3aa878c0-224b-41e5-abd1-63be359d2bca', 'score': 70, 'name': 'Nirvana', 'sort-name': 'Nirvana', 'disambiguation': 'founded in 1987 by a Michael Jackson double/imitator', 'life-span': {'begin': '1987', 'ended': None}}, {'id': 'f2dfdff9-3862-4be0-bf85-9c833fa3059e', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 63, 'name': 'Nirvana 2002', 'sort-name': 'Nirvana 2002', 'country': 'SE', 'area': {'id': '23d10872-f5ae-3f0c-bf55-332788a16ecb', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'Sweden', 'sort-name': 'Sweden', 'life-span': {'ended': None}}, 'disambiguation': 'Swedish death metal band', 'life-span': {'begin': '1988', 'end': '2012', 'ended': True}, 'aliases': [{'sort-name': 'Nirvana', 'name': 'Nirvana', 'locale': None, 'type': None, 'primary': None, 'begin-date': '1988', 'end-date': '1988'}, {'sort-name': 'Prophet 2002', 'name': 'Prophet 2002', 'locale': None, 'type': None, 'primary': None, 'begin-date': '1988', 'end-date': '1988'}, {'sort-name': 'N2K2', 'name': 'N2K2', 'locale': None, 'type': None, 'primary': None, 'begin-date': None, 'end-date': None}]}, {'id': '329c04ae-3b73-4ca3-996f-75608ab1befb', 'type': 'Person', 'type-id': 'b6e035f4-3ce9-331c-97df-83397230b0df', 'score': 62, 'name': 'Nirvana Singh', 'sort-name': 'Singh, Nirvana', 'life-span': {'ended': None}}, {'id': '86f9ae24-ba2a-4d55-9275-0b89b85f6e3a', 'score': 62, 'name': 'Weed Nirvana', 'sort-name': 'Weed Nirvana', 'life-span': {'ended': None}}, {'id': 'b305320e-c158-43f4-b5be-4450e2f99a32', 'score': 62, 'name': 'El Nirvana', 'sort-name': 'Nirvana, El', 'life-span': {'ended': None}}, {'id': '206419e0-3a7a-49ce-8437-4e757767d02b', 'type': 'Person', 'type-id': 'b6e035f4-3ce9-331c-97df-83397230b0df', 'score': 62, 'name': 'Nirvana Savoury', 'sort-name': 'Savoury, Nirvana', 'gender': 'female', 'country': 'US', 'area': {'id': '489ce91b-6658-3307-9877-795b68554c98', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'United States', 'sort-name': 'United States', 'life-span': {'ended': None}}, 'life-span': {'ended': None}}, {'id': 'f58febd3-18de-4371-8d95-4a68d4f79456', 'type': 'Person', 'type-id': 'b6e035f4-3ce9-331c-97df-83397230b0df', 'score': 62, 'name': 'Nirvana Kelly', 'sort-name': 'Kelly, Nirvana', 'life-span': {'ended': None}}, {'id': '8f32371e-4bac-4090-ba13-2c1cd1aeab0d', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 62, 'name': 'Nirvana UK', 'sort-name': 'Nirvana UK', 'area': {'id': '07607044-8140-47ba-bb24-7129babe586b', 'type': 'Subdivision', 'type-id': 'fd3d44c5-80a1-3842-9745-2c4972d35afa', 'name': 'West Midlands', 'sort-name': 'West Midlands', 'life-span': {'ended': None}}, 'life-span': {'ended': None}}, {'id': 'da77e424-c473-481e-a2ae-5d966b2ee0b6', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 62, 'name': 'Nirvana Undercover', 'sort-name': 'Nirvana Undercover', 'begin-area': {'id': 'ef1b7cc0-cd26-36f4-8ea0-04d9623786c7', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'Netherlands', 'sort-name': 'Netherlands', 'life-span': {'ended': None}}, 'life-span': {'ended': None}}, {'id': '02c4e6bb-7b7a-4686-8c23-df01bfd42b0e', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 56, 'name': 'Sappy Nirvana Tribute', 'sort-name': 'Sappy Nirvana Tribute', 'area': {'id': 'c621114d-73cc-4832-8afe-f13dc261e5af', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Gatineau', 'sort-name': 'Gatineau', 'life-span': {'ended': None}}, 'begin-area': {'id': 'c621114d-73cc-4832-8afe-f13dc261e5af', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Gatineau', 'sort-name': 'Gatineau', 'life-span': {'ended': None}}, 'life-span': {'begin': '2012-04-05', 'ended': None}}, {'id': '46d8dae4-abec-438b-9c62-a3dbb2aaa1b7', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 56, 'name': 'Nirvana Teen Spirit', 'sort-name': 'Nirvana Teen Spirit', 'area': {'id': 'e8ad73e9-9e7f-41c4-a395-6e29260ff1df', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Graz', 'sort-name': 'Graz', 'life-span': {'ended': None}}, 'begin-area': {'id': 'e8ad73e9-9e7f-41c4-a395-6e29260ff1df', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Graz', 'sort-name': 'Graz', 'life-span': {'ended': None}}, 'disambiguation': 'Nirvana-Coverband', 'life-span': {'begin': '2000', 'ended': None}}, {'id': 'e43ad11b-5d29-45ae-90f7-73ac47fb815d', 'score': 56, 'name': 'smells like nirvana', 'sort-name': 'smells like nirvana', 'life-span': {'ended': None}}, {'id': '45eacd92-6857-4faa-9283-023e72a1d4b1', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 56, 'name': 'The Nirvana Experience', 'sort-name': 'The Nirvana Experience', 'area': {'id': 'c920948b-83e3-40b7-8fe9-9ab5abaac55b', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Houston', 'sort-name': 'Houston', 'life-span': {'ended': None}}, 'begin-area': {'id': 'c920948b-83e3-40b7-8fe9-9ab5abaac55b', 'type': 'City', 'type-id': '6fd8f29a-3d0a-32fc-980d-ea697b69da78', 'name': 'Houston', 'sort-name': 'Houston', 'life-span': {'ended': None}}, 'disambiguation': 'Nirvana Cover Band', 'life-span': {'begin': '2012', 'ended': None}}, {'id': '1d512080-0ef7-454f-a325-9bf5ff95ce88', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 56, 'name': 'Come As Nirvana', 'sort-name': 'Come As Nirvana', 'country': 'BE', 'area': {'id': '5b8a5ee5-0bb3-34cf-9a75-c27c44e341fc', 'type': 'Country', 'type-id': '06dd0ae4-8c74-30bb-b43d-95dcedf961de', 'name': 'Belgium', 'sort-name': 'Belgium', 'life-span': {'ended': None}}, 'disambiguation': 'Belgian', 'life-span': {'ended': None}}, {'id': 'bb94730d-22c2-422d-a0a7-fe16a5b3e429', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 52, 'name': 'The Attainment of Nirvana', 'sort-name': 'Attainment of Nirvana, The', 'life-span': {'ended': None}}, {'id': 'e5a4c61b-1613-4841-8706-33d99b1e7ba5', 'type': 'Group', 'type-id': 'e431f5f6-b5d2-343d-8b36-72607fffb74b', 'score': 52, 'name': 'Hormoaning (Nirvana - SP)', 'sort-name': 'Hormoaning ( Nirvana - SP )', 'life-span': {'begin': '2007-06-01', 'ended': None}}, {'id': 'e1388435-f80d-434a-9980-f1c9f5aa9b90', 'score': 49, 'name': 'Nirvana Sitar & String Group', 'sort-name': 'Nirvana Sitar & String Group', 'life-span': {'ended': None}}]\n"
     ]
    }
   ],
   "source": [
    "results = query_by_name(artist_url, query_type[\"simple\"], \"Nirvana\")\n",
    "pretty_print(results['artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created', 'count', 'offset', 'artists'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\",\n",
      "        \"life-span\": {\n",
      "            \"ended\": null\n",
      "        },\n",
      "        \"name\": \"Finland\",\n",
      "        \"sort-name\": \"Finland\",\n",
      "        \"type\": \"Country\",\n",
      "        \"type-id\": \"06dd0ae4-8c74-30bb-b43d-95dcedf961de\"\n",
      "    },\n",
      "    \"country\": \"FI\",\n",
      "    \"disambiguation\": \"Early 1980's Finnish punk band\",\n",
      "    \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\",\n",
      "    \"life-span\": {\n",
      "        \"ended\": null\n",
      "    },\n",
      "    \"name\": \"Nirvana\",\n",
      "    \"score\": 71,\n",
      "    \"sort-name\": \"Nirvana\",\n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 1,\n",
      "            \"name\": \"punk\"\n",
      "        },\n",
      "        {\n",
      "            \"count\": 1,\n",
      "            \"name\": \"finland\"\n",
      "        }\n",
      "    ],\n",
      "    \"type\": \"Group\",\n",
      "    \"type-id\": \"e431f5f6-b5d2-343d-8b36-72607fffb74b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nARTIST:\")\n",
    "pretty_print(results[\"artists\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85af0709-95db-4fbc-801a-120e9f4766d0'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_id = results[\"artists\"][4][\"id\"]\n",
    "artist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/85af0709-95db-4fbc-801a-120e9f4766d0?inc=releases&fmt=json\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": \"\",\n",
      "  \"country\": \"FI\",\n",
      "  \"date\": \"1980\",\n",
      "  \"disambiguation\": \"\",\n",
      "  \"id\": \"3e25396c-5c66-4609-8e47-37f250d323c7\",\n",
      "  \"packaging\": \"Cardboard/Paper Sleeve\",\n",
      "  \"packaging-id\": \"f7101ce3-0384-39ce-9fde-fbbd0044d35f\",\n",
      "  \"quality\": \"normal\",\n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\",\n",
      "        \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\",\n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"FI\"\n",
      "        ],\n",
      "        \"name\": \"Finland\",\n",
      "        \"sort-name\": \"Finland\"\n",
      "      },\n",
      "      \"date\": \"1980\"\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"Official\",\n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\",\n",
      "  \"text-representation\": {\n",
      "    \"language\": \"fin\",\n",
      "    \"script\": \"Latn\"\n",
      "  },\n",
      "  \"title\": \"Nirvana\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "artist_data = query_site(artist_url, query_type[\"releases\"], artist_id)\n",
    "releases = artist_data[\"releases\"]\n",
    "print(\"\\nONE RELEASE:\")\n",
    "pretty_print(releases[0], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'packaging-id': 'f7101ce3-0384-39ce-9fde-fbbd0044d35f',\n",
       "  'status': 'Official',\n",
       "  'quality': 'normal',\n",
       "  'barcode': '',\n",
       "  'packaging': 'Cardboard/Paper Sleeve',\n",
       "  'disambiguation': '',\n",
       "  'date': '1980',\n",
       "  'release-events': [{'area': {'sort-name': 'Finland',\n",
       "     'name': 'Finland',\n",
       "     'id': '6a264f94-6ff1-30b1-9a81-41f7bfabd616',\n",
       "     'disambiguation': '',\n",
       "     'iso-3166-1-codes': ['FI']},\n",
       "    'date': '1980'}],\n",
       "  'text-representation': {'language': 'fin', 'script': 'Latn'},\n",
       "  'title': 'Nirvana',\n",
       "  'id': '3e25396c-5c66-4609-8e47-37f250d323c7',\n",
       "  'status-id': '4e304316-386d-3409-af2e-78857eec5cfe',\n",
       "  'country': 'FI'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ALL TITLES:\n",
      "Nirvana\n"
     ]
    }
   ],
   "source": [
    "release_titles = [r[\"title\"] for r in releases]\n",
    "print(\"\\nALL TITLES:\")\n",
    "for t in release_titles:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>assignee</th>\n",
       "      <th>assignees</th>\n",
       "      <th>milestone</th>\n",
       "      <th>comments</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>body</th>\n",
       "      <th>pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://github.com/pandas-dev/pandas/issues/29835</td>\n",
       "      <td>528170077</td>\n",
       "      <td>MDU6SXNzdWU1MjgxNzAwNzc=</td>\n",
       "      <td>29835</td>\n",
       "      <td>deleting index column but index=False not working</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-25 15:43:35+00:00</td>\n",
       "      <td>2019-11-25 15:43:35+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>#### Code Sample, a copy-pastable example if p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://github.com/pandas-dev/pandas/issues/29834</td>\n",
       "      <td>528129533</td>\n",
       "      <td>MDU6SXNzdWU1MjgxMjk1MzM=</td>\n",
       "      <td>29834</td>\n",
       "      <td>pivot_table with MultiIndex and margin=True fa...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-25 14:38:06+00:00</td>\n",
       "      <td>2019-11-25 14:38:06+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>#### Code Sample, a copy-pastable example if p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://github.com/pandas-dev/pandas/issues/29833</td>\n",
       "      <td>527889585</td>\n",
       "      <td>MDU6SXNzdWU1Mjc4ODk1ODU=</td>\n",
       "      <td>29833</td>\n",
       "      <td>Aggregation fails when grouping on more than o...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-25 07:02:02+00:00</td>\n",
       "      <td>2019-11-25 11:20:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>#### Code Sample, a copy-pastable example if p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://github.com/pandas-dev/pandas/issues/29832</td>\n",
       "      <td>527872894</td>\n",
       "      <td>MDU6SXNzdWU1Mjc4NzI4OTQ=</td>\n",
       "      <td>29832</td>\n",
       "      <td>read_hdf() file name encoding with with accent...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-25 06:16:51+00:00</td>\n",
       "      <td>2019-11-25 12:12:46+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>#### Problem description\\r\\n\\r\\npd.read_hdf() ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://api.github.com/repos/pandas-dev/pandas...</td>\n",
       "      <td>https://github.com/pandas-dev/pandas/pull/29831</td>\n",
       "      <td>527812463</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0MzQ0OTg5Njk1</td>\n",
       "      <td>29831</td>\n",
       "      <td>DEPR: Remove weekday_name</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-25 02:50:41+00:00</td>\n",
       "      <td>2019-11-25 15:46:44+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>- [x] xref #18164\\r\\n- [x] tests added / passe...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/pandas-d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "\n",
       "                                   repository_url  \\\n",
       "0  https://api.github.com/repos/pandas-dev/pandas   \n",
       "1  https://api.github.com/repos/pandas-dev/pandas   \n",
       "2  https://api.github.com/repos/pandas-dev/pandas   \n",
       "3  https://api.github.com/repos/pandas-dev/pandas   \n",
       "4  https://api.github.com/repos/pandas-dev/pandas   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
       "\n",
       "                                            html_url         id  \\\n",
       "0  https://github.com/pandas-dev/pandas/issues/29835  528170077   \n",
       "1  https://github.com/pandas-dev/pandas/issues/29834  528129533   \n",
       "2  https://github.com/pandas-dev/pandas/issues/29833  527889585   \n",
       "3  https://github.com/pandas-dev/pandas/issues/29832  527872894   \n",
       "4    https://github.com/pandas-dev/pandas/pull/29831  527812463   \n",
       "\n",
       "                            node_id  number  \\\n",
       "0          MDU6SXNzdWU1MjgxNzAwNzc=   29835   \n",
       "1          MDU6SXNzdWU1MjgxMjk1MzM=   29834   \n",
       "2          MDU6SXNzdWU1Mjc4ODk1ODU=   29833   \n",
       "3          MDU6SXNzdWU1Mjc4NzI4OTQ=   29832   \n",
       "4  MDExOlB1bGxSZXF1ZXN0MzQ0OTg5Njk1   29831   \n",
       "\n",
       "                                               title  ... assignee assignees  \\\n",
       "0  deleting index column but index=False not working  ...      NaN        []   \n",
       "1  pivot_table with MultiIndex and margin=True fa...  ...      NaN        []   \n",
       "2  Aggregation fails when grouping on more than o...  ...      NaN        []   \n",
       "3  read_hdf() file name encoding with with accent...  ...      NaN        []   \n",
       "4                          DEPR: Remove weekday_name  ...      NaN        []   \n",
       "\n",
       "  milestone  comments                created_at                updated_at  \\\n",
       "0       NaN         0 2019-11-25 15:43:35+00:00 2019-11-25 15:43:35+00:00   \n",
       "1       NaN         0 2019-11-25 14:38:06+00:00 2019-11-25 14:38:06+00:00   \n",
       "2       NaN         1 2019-11-25 07:02:02+00:00 2019-11-25 11:20:00+00:00   \n",
       "3       NaN         1 2019-11-25 06:16:51+00:00 2019-11-25 12:12:46+00:00   \n",
       "4       NaN         1 2019-11-25 02:50:41+00:00 2019-11-25 15:46:44+00:00   \n",
       "\n",
       "   closed_at  author_association  \\\n",
       "0        NaT                NONE   \n",
       "1        NaT                NONE   \n",
       "2        NaT                NONE   \n",
       "3        NaT                NONE   \n",
       "4        NaT              MEMBER   \n",
       "\n",
       "                                                body  \\\n",
       "0  #### Code Sample, a copy-pastable example if p...   \n",
       "1  #### Code Sample, a copy-pastable example if p...   \n",
       "2  #### Code Sample, a copy-pastable example if p...   \n",
       "3  #### Problem description\\r\\n\\r\\npd.read_hdf() ...   \n",
       "4  - [x] xref #18164\\r\\n- [x] tests added / passe...   \n",
       "\n",
       "                                        pull_request  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  {'url': 'https://api.github.com/repos/pandas-d...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')\n",
    "df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'martin': {'name': \"Martin D'vloper\", 'job': 'Developer', 'skills': ['python', 'perl', 'pascal']}}, {'tabitha': {'name': 'Tabitha Bitumen', 'job': 'Developer', 'skills': ['lisp', 'fortran', 'erlang']}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/Documents/envs/python_env/lib/python3.6/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "with open(datapath / 'simple.yaml', 'r') as stream:\n",
    "    try:\n",
    "        print(yaml.load(stream))\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/urllib.html\n",
    "\n",
    "https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "\n",
    "http://lxml.de/\n",
    "\n",
    "http://www.w3schools.com/xml/xml_examples.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "#from xml.etree import ElementTree as ET\n",
    "from lxml import etree as ET #Supports xpath syntax\n",
    "from collections import defaultdict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.w3schools.com/xml/cd_catalog.xml'\n",
    "xml_page = urllib.request.urlopen(url).read()\n",
    "e = ET.XML(xml_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etree_to_dict(t):\n",
    "    d = {t.tag: {} if t.attrib else None}\n",
    "    children = list(t)\n",
    "    if children:\n",
    "        dd = defaultdict(list)\n",
    "        for dc in map(etree_to_dict, children):\n",
    "            for k, v in dc.items():\n",
    "                dd[k].append(v)\n",
    "        d = {t.tag: {k:v[0] if len(v) == 1 else v for k, v in dd.items()}}\n",
    "    if t.attrib:\n",
    "        d[t.tag].update(('@' + k, v) for k, v in t.attrib.items())\n",
    "    if t.text:\n",
    "        text = t.text.strip()\n",
    "        if children or t.attrib:\n",
    "            if text:\n",
    "              d[t.tag]['#text'] = text\n",
    "        else:\n",
    "            d[t.tag] = text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CATALOG': {'CD': [{'ARTIST': 'Bob Dylan',\n",
      "                     'COMPANY': 'Columbia',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '10.90',\n",
      "                     'TITLE': 'Empire Burlesque',\n",
      "                     'YEAR': '1985'},\n",
      "                    {'ARTIST': 'Bonnie Tyler',\n",
      "                     'COMPANY': 'CBS Records',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '9.90',\n",
      "                     'TITLE': 'Hide your heart',\n",
      "                     'YEAR': '1988'},\n",
      "                    {'ARTIST': 'Dolly Parton',\n",
      "                     'COMPANY': 'RCA',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '9.90',\n",
      "                     'TITLE': 'Greatest Hits',\n",
      "                     'YEAR': '1982'},\n",
      "                    {'ARTIST': 'Gary Moore',\n",
      "                     'COMPANY': 'Virgin records',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '10.20',\n",
      "                     'TITLE': 'Still got the blues',\n",
      "                     'YEAR': '1990'},\n",
      "                    {'ARTIST': 'Eros Ramazzotti',\n",
      "                     'COMPANY': 'BMG',\n",
      "                     'COUNTRY': 'EU',\n",
      "                     'PRICE': '9.90',\n",
      "                     'TITLE': 'Eros',\n",
      "                     'YEAR': '1997'},\n",
      "                    {'ARTIST': 'Bee Gees',\n",
      "                     'COMPANY': 'Polydor',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '10.90',\n",
      "                     'TITLE': 'One night only',\n",
      "                     'YEAR': '1998'},\n",
      "                    {'ARTIST': 'Dr.Hook',\n",
      "                     'COMPANY': 'CBS',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.10',\n",
      "                     'TITLE': 'Sylvias Mother',\n",
      "                     'YEAR': '1973'},\n",
      "                    {'ARTIST': 'Rod Stewart',\n",
      "                     'COMPANY': 'Pickwick',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.50',\n",
      "                     'TITLE': 'Maggie May',\n",
      "                     'YEAR': '1990'},\n",
      "                    {'ARTIST': 'Andrea Bocelli',\n",
      "                     'COMPANY': 'Polydor',\n",
      "                     'COUNTRY': 'EU',\n",
      "                     'PRICE': '10.80',\n",
      "                     'TITLE': 'Romanza',\n",
      "                     'YEAR': '1996'},\n",
      "                    {'ARTIST': 'Percy Sledge',\n",
      "                     'COMPANY': 'Atlantic',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '8.70',\n",
      "                     'TITLE': 'When a man loves a woman',\n",
      "                     'YEAR': '1987'},\n",
      "                    {'ARTIST': 'Savage Rose',\n",
      "                     'COMPANY': 'Mega',\n",
      "                     'COUNTRY': 'EU',\n",
      "                     'PRICE': '10.90',\n",
      "                     'TITLE': 'Black angel',\n",
      "                     'YEAR': '1995'},\n",
      "                    {'ARTIST': 'Many',\n",
      "                     'COMPANY': 'Grammy',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '10.20',\n",
      "                     'TITLE': '1999 Grammy Nominees',\n",
      "                     'YEAR': '1999'},\n",
      "                    {'ARTIST': 'Kenny Rogers',\n",
      "                     'COMPANY': 'Mucik Master',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.70',\n",
      "                     'TITLE': 'For the good times',\n",
      "                     'YEAR': '1995'},\n",
      "                    {'ARTIST': 'Will Smith',\n",
      "                     'COMPANY': 'Columbia',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '9.90',\n",
      "                     'TITLE': 'Big Willie style',\n",
      "                     'YEAR': '1997'},\n",
      "                    {'ARTIST': 'Van Morrison',\n",
      "                     'COMPANY': 'Polydor',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.20',\n",
      "                     'TITLE': 'Tupelo Honey',\n",
      "                     'YEAR': '1971'},\n",
      "                    {'ARTIST': 'Jorn Hoel',\n",
      "                     'COMPANY': 'WEA',\n",
      "                     'COUNTRY': 'Norway',\n",
      "                     'PRICE': '7.90',\n",
      "                     'TITLE': 'Soulsville',\n",
      "                     'YEAR': '1996'},\n",
      "                    {'ARTIST': 'Cat Stevens',\n",
      "                     'COMPANY': 'Island',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.90',\n",
      "                     'TITLE': 'The very best of',\n",
      "                     'YEAR': '1990'},\n",
      "                    {'ARTIST': 'Sam Brown',\n",
      "                     'COMPANY': 'A and M',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.90',\n",
      "                     'TITLE': 'Stop',\n",
      "                     'YEAR': '1988'},\n",
      "                    {'ARTIST': \"T'Pau\",\n",
      "                     'COMPANY': 'Siren',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '7.90',\n",
      "                     'TITLE': 'Bridge of Spies',\n",
      "                     'YEAR': '1987'},\n",
      "                    {'ARTIST': 'Tina Turner',\n",
      "                     'COMPANY': 'Capitol',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '8.90',\n",
      "                     'TITLE': 'Private Dancer',\n",
      "                     'YEAR': '1983'},\n",
      "                    {'ARTIST': 'Kim Larsen',\n",
      "                     'COMPANY': 'Medley',\n",
      "                     'COUNTRY': 'EU',\n",
      "                     'PRICE': '7.80',\n",
      "                     'TITLE': 'Midt om natten',\n",
      "                     'YEAR': '1983'},\n",
      "                    {'ARTIST': 'Luciano Pavarotti',\n",
      "                     'COMPANY': 'DECCA',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '9.90',\n",
      "                     'TITLE': 'Pavarotti Gala Concert',\n",
      "                     'YEAR': '1991'},\n",
      "                    {'ARTIST': 'Otis Redding',\n",
      "                     'COMPANY': 'Stax Records',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '7.90',\n",
      "                     'TITLE': 'The dock of the bay',\n",
      "                     'YEAR': '1968'},\n",
      "                    {'ARTIST': 'Simply Red',\n",
      "                     'COMPANY': 'Elektra',\n",
      "                     'COUNTRY': 'EU',\n",
      "                     'PRICE': '7.20',\n",
      "                     'TITLE': 'Picture book',\n",
      "                     'YEAR': '1985'},\n",
      "                    {'ARTIST': 'The Communards',\n",
      "                     'COMPANY': 'London',\n",
      "                     'COUNTRY': 'UK',\n",
      "                     'PRICE': '7.80',\n",
      "                     'TITLE': 'Red',\n",
      "                     'YEAR': '1987'},\n",
      "                    {'ARTIST': 'Joe Cocker',\n",
      "                     'COMPANY': 'EMI',\n",
      "                     'COUNTRY': 'USA',\n",
      "                     'PRICE': '8.20',\n",
      "                     'TITLE': 'Unchain my heart',\n",
      "                     'YEAR': '1987'}]}}\n"
     ]
    }
   ],
   "source": [
    "pprint(etree_to_dict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element node: CATALOG\n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Empire Burlesque\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Bob Dylan\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Columbia\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1985\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Hide your heart\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Bonnie Tyler\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:CBS Records\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:9.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1988\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Greatest Hits\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Dolly Parton\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:RCA\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:9.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1982\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Still got the blues\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Gary Moore\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Virgin records\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.20\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1990\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Eros\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Eros Ramazzotti\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:EU\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:BMG\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:9.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1997\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:One night only\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Bee Gees\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Polydor\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1998\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Sylvias Mother\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Dr.Hook\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:CBS\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.10\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1973\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Maggie May\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Rod Stewart\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Pickwick\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.50\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1990\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Romanza\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Andrea Bocelli\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:EU\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Polydor\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.80\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1996\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:When a man loves a woman\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Percy Sledge\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Atlantic\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.70\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1987\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Black angel\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Savage Rose\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:EU\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Mega\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1995\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:1999 Grammy Nominees\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Many\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Grammy\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:10.20\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1999\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:For the good times\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Kenny Rogers\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Mucik Master\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.70\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1995\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Big Willie style\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Will Smith\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Columbia\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:9.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1997\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Tupelo Honey\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Van Morrison\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Polydor\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.20\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1971\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Soulsville\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Jorn Hoel\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:Norway\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:WEA\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1996\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:The very best of\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Cat Stevens\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Island\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1990\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Stop\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Sam Brown\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:A and M\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1988\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Bridge of Spies\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:T'Pau\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Siren\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1987\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Private Dancer\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Tina Turner\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Capitol\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1983\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Midt om natten\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Kim Larsen\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:EU\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Medley\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.80\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1983\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Pavarotti Gala Concert\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Luciano Pavarotti\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:DECCA\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:9.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1991\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:The dock of the bay\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Otis Redding\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Stax Records\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.90\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1968\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Picture book\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Simply Red\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:EU\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:Elektra\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.20\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1985\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Red\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:The Communards\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:UK\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:London\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:7.80\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1987\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "  \n",
      "Element node: CD\n",
      "text node:\n",
      "    \n",
      "Element node: TITLE\n",
      "text node:Unchain my heart\n",
      "text node:\n",
      "    \n",
      "Element node: ARTIST\n",
      "text node:Joe Cocker\n",
      "text node:\n",
      "    \n",
      "Element node: COUNTRY\n",
      "text node:USA\n",
      "text node:\n",
      "    \n",
      "Element node: COMPANY\n",
      "text node:EMI\n",
      "text node:\n",
      "    \n",
      "Element node: PRICE\n",
      "text node:8.20\n",
      "text node:\n",
      "    \n",
      "Element node: YEAR\n",
      "text node:1987\n",
      "text node:\n",
      "  \n",
      "text node:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_page = urllib.request.urlopen(url)\n",
    "doc = minidom.parse(raw_page)\n",
    "\n",
    "def findTextnodes(nodeList):\n",
    "    for subnode in nodeList:\n",
    "        if subnode.nodeType == subnode.ELEMENT_NODE:\n",
    "            print(\"Element node: \" + subnode.tagName)\n",
    "            findTextnodes(subnode.childNodes)\n",
    "        elif subnode.nodeType == subnode.TEXT_NODE:\n",
    "            print(\"text node:\" + subnode.data)\n",
    "            \n",
    "findTextnodes(doc.childNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire Burlesque\n",
      "Hide your heart\n",
      "Greatest Hits\n",
      "Still got the blues\n",
      "Eros\n",
      "One night only\n",
      "Sylvias Mother\n",
      "Maggie May\n",
      "Romanza\n",
      "When a man loves a woman\n",
      "Black angel\n",
      "1999 Grammy Nominees\n",
      "For the good times\n",
      "Big Willie style\n",
      "Tupelo Honey\n",
      "Soulsville\n",
      "The very best of\n",
      "Stop\n",
      "Bridge of Spies\n",
      "Private Dancer\n",
      "Midt om natten\n",
      "Pavarotti Gala Concert\n",
      "The dock of the bay\n",
      "Picture book\n",
      "Red\n",
      "Unchain my heart\n"
     ]
    }
   ],
   "source": [
    "root = ET.fromstring(xml_page)\n",
    "cdtags = root.xpath('//CD/TITLE')\n",
    "for cd in cdtags:\n",
    "    print(cd.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Dylan\tUSA\tEmpire Burlesque\n",
      "Bonnie Tyler\tUK\tHide your heart\n",
      "Dolly Parton\tUSA\tGreatest Hits\n",
      "Gary Moore\tUK\tStill got the blues\n",
      "Eros Ramazzotti\tEU\tEros\n",
      "Bee Gees\tUK\tOne night only\n",
      "Dr.Hook\tUK\tSylvias Mother\n",
      "Rod Stewart\tUK\tMaggie May\n",
      "Andrea Bocelli\tEU\tRomanza\n",
      "Percy Sledge\tUSA\tWhen a man loves a woman\n",
      "Savage Rose\tEU\tBlack angel\n",
      "Many\tUSA\t1999 Grammy Nominees\n",
      "Kenny Rogers\tUK\tFor the good times\n",
      "Will Smith\tUSA\tBig Willie style\n",
      "Van Morrison\tUK\tTupelo Honey\n",
      "Jorn Hoel\tNorway\tSoulsville\n",
      "Cat Stevens\tUK\tThe very best of\n",
      "Sam Brown\tUK\tStop\n",
      "T'Pau\tUK\tBridge of Spies\n",
      "Tina Turner\tUK\tPrivate Dancer\n",
      "Kim Larsen\tEU\tMidt om natten\n",
      "Luciano Pavarotti\tUK\tPavarotti Gala Concert\n",
      "Otis Redding\tUSA\tThe dock of the bay\n",
      "Simply Red\tEU\tPicture book\n",
      "The Communards\tUK\tRed\n",
      "Joe Cocker\tUSA\tUnchain my heart\n"
     ]
    }
   ],
   "source": [
    "for cd in root.findall('CD'):\n",
    "    title = cd.find('TITLE').text\n",
    "    artist = cd.find('ARTIST').text\n",
    "    country = cd.find('COUNTRY').text\n",
    "    print('{}\\t{}\\t{}'.format(artist, country, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TITLE': 'Empire Burlesque',\n",
       "  'ARTIST': 'Bob Dylan',\n",
       "  'COUNTRY': 'USA',\n",
       "  'PRICE': '10.90',\n",
       "  'YEAR': '1985'},\n",
       " {'TITLE': 'Hide your heart',\n",
       "  'ARTIST': 'Bonnie Tyler',\n",
       "  'COUNTRY': 'UK',\n",
       "  'PRICE': '9.90',\n",
       "  'YEAR': '1988'},\n",
       " {'TITLE': 'Greatest Hits',\n",
       "  'ARTIST': 'Dolly Parton',\n",
       "  'COUNTRY': 'USA',\n",
       "  'PRICE': '9.90',\n",
       "  'YEAR': '1982'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cds = []\n",
    "for cd in root.findall('./CD'):\n",
    "        data = {}\n",
    "        data[\"TITLE\"] = cd.find('./TITLE').text\n",
    "        data[\"ARTIST\"] = cd.find('./ARTIST').text\n",
    "        data[\"COUNTRY\"] = cd.find('./COUNTRY').text\n",
    "        data[\"PRICE\"] = cd.find('./PRICE').text\n",
    "        data[\"YEAR\"] = cd.find('./YEAR').text\n",
    "        cds.append(data)\n",
    "cds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing [SQLite](https://docs.python.org/3/library/sqlite3.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(os.path.join(outputs,'example.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS stocks\n",
    "             (date text, trans text, symbol text, qty real, price real)''')\n",
    "\n",
    "# Insert a row of data\n",
    "cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "\n",
    "# Save (commit) the changes\n",
    "conn.commit()\n",
    "\n",
    "# We can also close the connection if we are done with it.\n",
    "# Just be sure any changes have been committed or they will be lost.\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(os.path.join(outputs,'example.db'))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stocks',)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n"
     ]
    }
   ],
   "source": [
    "t = ('RHAT',) #tuple with just one element\n",
    "cur.execute('SELECT * FROM stocks WHERE symbol=?', t)\n",
    "print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fce47f971f0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger example that inserts many records at a time\n",
    "purchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n",
    "             ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n",
    "             ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n",
    "            ]\n",
    "cur.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n",
      "('2006-03-28', 'BUY', 'IBM', 1000.0, 45.0)\n",
      "('2006-04-06', 'SELL', 'IBM', 500.0, 53.0)\n",
      "('2006-04-05', 'BUY', 'MSFT', 1000.0, 72.0)\n"
     ]
    }
   ],
   "source": [
    "for row in cur.execute('SELECT * FROM stocks ORDER BY price'):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date trans symbol    qty  price\n",
      "0  2006-01-05   BUY   RHAT  100.0  35.14\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(os.path.join(outputs,'example.db'))\n",
    "df = pd.read_sql_query(\"SELECT * from stocks ORDER BY price\", conn)\n",
    "\n",
    "# verify that result of SQL query is stored in the dataframe\n",
    "print(df.head())\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the database file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(os.path.join(outputs,'example.db'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing [MySQL](https://pypi.python.org/pypi/PyMySQL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getpass.getpass()\n",
    "conn = pymysql.connect(host='localhost', port=3306, user='root', passwd=p,)# db='example')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"show databases;\")\n",
    "for r in cur.fetchall():\n",
    "   print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"use mysql;\")\n",
    "cur.execute(\"show tables;\")\n",
    "for r in cur.fetchall():\n",
    "   print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE DATABASE IF NOT EXISTS test;\")\n",
    "cur.execute(\"USE test;\")\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS test_table \n",
    "               (text VARCHAR(200)\n",
    "               DEFAULT NULL)\n",
    "               ENGINE=MyISAM DEFAULT CHARSET='utf8';''')\n",
    "\n",
    "cur.execute(\"show tables;\")\n",
    "for r in cur.fetchall():\n",
    "   print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['It always seems impossible until its done.',\n",
    "         'In order to succeed, we must first believe that we can.',\n",
    "         'Life is 10% what happens to you and 90% how you react to it.',\n",
    "         'Start where you are. Use what you have. Do what you can.',]\n",
    "cur.executemany('INSERT INTO test.test_table (text) VALUES (%s)', texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM test.test_table;\")# LIMIT 10;\")\n",
    "for r in cur.fetchall():\n",
    "   print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.io.sql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mysql = psql.read_sql('select * from test.test_table;', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mysql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing [PostgreSQL](https://www.psycopg.org/docs/)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getpass.getpass()\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='testdb' user='rsouza' host='localhost' password='{}'\".format(p))\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM postgres LIMIT 2;')\n",
    "for r in cur.fetchall():\n",
    "   print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getpass.getpass()\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='postgres' user='rsouza' host='localhost' password='{}'\".format(p))\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postgres = psql.read_sql('SELECT * FROM postgres LIMIT 5;', con=conn)\n",
    "df_postgres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing Mongo DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://api.mongodb.org/python/current/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "#client = MongoClient('localhost', 27017)\n",
    "db = client.test_database #acessa ou cria o banco\n",
    "\n",
    "## dropping a database via pymongo\n",
    "#client.drop_database('test_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = {\"author\": \"Renato\",\n",
    "        \"text\": \"My first blog post!\",\n",
    "        \"tags\": [\"mongodb\", \"python\", \"pymongo\"],\n",
    "        \"date\": datetime.datetime.utcnow()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = db.posts # cria a coleção\n",
    "post_id = posts.insert_one(post) #insere dados\n",
    "post_id\n",
    "\n",
    "## drop a collection via pymongo\n",
    "#client['test_database'].drop_collection('posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts = [{\"author\": \"Mike\",\n",
    "              \"text\": \"Another post!\",\n",
    "              \"tags\": [\"bulk\", \"insert\"],\n",
    "              \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n",
    "             {\"author\": \"Eliot\",\n",
    "              \"title\": \"MongoDB is fun\",\n",
    "              \"text\": \"and pretty easy too!\",\n",
    "              \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.insert_many(new_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.find_one({\"author\": \"Mike\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for post in posts.find({\"author\": \"Mike\"}):\n",
    "for post in posts.find():\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pd = posts.find()\n",
    "df_pandas =  pd.DataFrame(list(example_pd))\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Feeds RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.feedparser.org/  \n",
    "http://docs.python.org/library/re.html  \n",
    "http://www.pythonware.com/library/pil/handbook/index.htm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = feedparser.parse('https://oglobo.globo.com/rss.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['feed']['title'])\n",
    "print(d['feed']['link'])\n",
    "#print(d.feed.subtitle)\n",
    "print(len(d['entries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['entries'][0]['title']) \n",
    "print(d.entries[0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in d.entries:\n",
    "    print(post.title + \": \" + post.link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bear/python-twitter  \n",
    "Go to http://twitter.com/apps/new to create an app and get these items  \n",
    "See https://dev.twitter.com/docs/auth/oauth for more information on Twitter's OAuth implementation  \n",
    "https://dev.twitter.com/rest/reference/get/account/verify_credentials  \n",
    "https://dev.twitter.com/oauth/overview/application-owner-access-tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Documents/MMD/notebooks/twitter_tokens.txt', 'r') as twitter_tokens:\n",
    "    tokens = twitter_tokens.read().split(',')\n",
    "consumer_key = tokens[0].strip()\n",
    "consumer_secret = tokens[1].strip()\n",
    "access_token = tokens[2].strip()\n",
    "access_token_secret = tokens[3].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct, sign, and open a twitter request using the hard-coded credentials above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=consumer_key, consumer_secret=consumer_secret, \n",
    "                  access_token_key=access_token, access_token_secret=access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = api.GetFriends()\n",
    "print([u.name for u in users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = api.GetFollowers()\n",
    "print([u.screen_name for u in followers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = api.GetMentions()\n",
    "#mentions[0].text\n",
    "print([m.text for m in mentions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = api.GetUserTimeline(screen_name='TIME')\n",
    "print([s.text for s in statuses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Big Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://eforexcel.com/wp/wp-content/uploads/2017/07/1500000%20Sales%20Records.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../datasets/CSVs/bigcsv.zip', <http.client.HTTPMessage at 0x7fbc4ebe47f0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(link, \"../datasets/CSVs/bigcsv.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pandas with chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving a new chunk\n",
      "                         Region           Country Item Type Sales Channel  \\\n",
      "0            Sub-Saharan Africa      South Africa    Fruits       Offline   \n",
      "1  Middle East and North Africa           Morocco   Clothes        Online   \n",
      "2         Australia and Oceania  Papua New Guinea      Meat       Offline   \n",
      "\n",
      "  Order Priority Order Date   Order ID   Ship Date  Units Sold  Unit Price  \\\n",
      "0              M  7/27/2012  443368995   7/28/2012        1593        9.33   \n",
      "1              M  9/14/2013  667593514  10/19/2013        4611      109.28   \n",
      "2              M  5/15/2015  940995585    6/4/2015         360      421.89   \n",
      "\n",
      "   Unit Cost  Total Revenue  Total Cost  Total Profit  \n",
      "0       6.92       14862.69    11023.56       3839.13  \n",
      "1      35.84      503890.08   165258.24     338631.84  \n",
      "2     364.69      151880.40   131288.40      20592.00  \n",
      "\n",
      "Retrieving a new chunk\n",
      "                       Region      Country      Item Type Sales Channel  \\\n",
      "100000     Sub-Saharan Africa   Madagascar         Cereal       Offline   \n",
      "100001  Australia and Oceania  New Zealand  Personal Care       Offline   \n",
      "100002     Sub-Saharan Africa       Malawi         Cereal       Offline   \n",
      "\n",
      "       Order Priority Order Date   Order ID  Ship Date  Units Sold  \\\n",
      "100000              L   1/9/2016  238872230  1/24/2016        6456   \n",
      "100001              C  6/14/2014  976817452  7/21/2014         846   \n",
      "100002              C  4/27/2017  371752727  6/10/2017        8748   \n",
      "\n",
      "        Unit Price  Unit Cost  Total Revenue  Total Cost  Total Profit  \n",
      "100000      205.70     117.11     1327999.20   756062.16     571937.04  \n",
      "100001       81.73      56.67       69143.58    47942.82      21200.76  \n",
      "100002      205.70     117.11     1799463.60  1024478.28     774985.32  \n",
      "\n",
      "Retrieving a new chunk\n",
      "                                   Region         Country        Item Type  \\\n",
      "200000                             Europe  Czech Republic  Office Supplies   \n",
      "200001  Central America and the Caribbean        Honduras           Snacks   \n",
      "200002       Middle East and North Africa    Saudi Arabia           Cereal   \n",
      "\n",
      "       Sales Channel Order Priority Order Date   Order ID   Ship Date  \\\n",
      "200000       Offline              M  8/30/2011  457898414   8/31/2011   \n",
      "200001        Online              L  12/6/2014  820111215  12/17/2014   \n",
      "200002       Offline              M  3/28/2014  134626567    4/2/2014   \n",
      "\n",
      "        Units Sold  Unit Price  Unit Cost  Total Revenue  Total Cost  \\\n",
      "200000        1171      651.21     524.96      762566.91   614728.16   \n",
      "200001        7988      152.58      97.44     1218809.04   778350.72   \n",
      "200002        7848      205.70     117.11     1614333.60   919079.28   \n",
      "\n",
      "        Total Profit  \n",
      "200000     147838.75  \n",
      "200001     440458.32  \n",
      "200002     695254.32  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10 ** 5\n",
    "num_chunks_to_read = 3\n",
    "\n",
    "with ZipFile(\"../datasets/CSVs/bigcsv.zip\", 'r') as myzip:\n",
    "    with myzip.open('1500000 Sales Records.csv') as myfile:\n",
    "        chunks = pd.read_csv((myfile), chunksize=chunksize) #, iterator=True)\n",
    "        for i in range(num_chunks_to_read):\n",
    "            print('Retrieving a new chunk')\n",
    "            df = chunks.get_chunk()\n",
    "            print(df.head(3))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping the file and reading line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"../datasets/CSVs/bigcsv.zip\", 'r') as myzip:\n",
    "    myzip.extractall(\"../datasets/CSVs/\")\n",
    "    myzip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstuff(filename):\n",
    "    with open(filename, \"r\") as csvfile:\n",
    "        datareader = csv.reader(csvfile)\n",
    "        count = 0\n",
    "        for row in datareader:\n",
    "            yield(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Region', 'Country', 'Item Type', 'Sales Channel', 'Order Priority', 'Order Date', 'Order ID', 'Ship Date', 'Units Sold', 'Unit Price', 'Unit Cost', 'Total Revenue', 'Total Cost', 'Total Profit']\n",
      "['Sub-Saharan Africa', 'South Africa', 'Fruits', 'Offline', 'M', '7/27/2012', '443368995', '7/28/2012', '1593', '9.33', '6.92', '14862.69', '11023.56', '3839.13']\n",
      "['Middle East and North Africa', 'Morocco', 'Clothes', 'Online', 'M', '9/14/2013', '667593514', '10/19/2013', '4611', '109.28', '35.84', '503890.08', '165258.24', '338631.84']\n",
      "['Australia and Oceania', 'Papua New Guinea', 'Meat', 'Offline', 'M', '5/15/2015', '940995585', '6/4/2015', '360', '421.89', '364.69', '151880.40', '131288.40', '20592.00']\n",
      "['Sub-Saharan Africa', 'Djibouti', 'Clothes', 'Offline', 'H', '5/17/2017', '880811536', '7/2/2017', '562', '109.28', '35.84', '61415.36', '20142.08', '41273.28']\n",
      "['Europe', 'Slovakia', 'Beverages', 'Offline', 'L', '10/26/2016', '174590194', '12/4/2016', '3973', '47.45', '31.79', '188518.85', '126301.67', '62217.18']\n",
      "['Asia', 'Sri Lanka', 'Fruits', 'Online', 'L', '11/7/2011', '830192887', '12/18/2011', '1379', '9.33', '6.92', '12866.07', '9542.68', '3323.39']\n",
      "['Sub-Saharan Africa', 'Seychelles ', 'Beverages', 'Online', 'M', '1/18/2013', '425793445', '2/16/2013', '597', '47.45', '31.79', '28327.65', '18978.63', '9349.02']\n",
      "['Sub-Saharan Africa', 'Tanzania', 'Beverages', 'Online', 'L', '11/30/2016', '659878194', '1/16/2017', '1476', '47.45', '31.79', '70036.20', '46922.04', '23114.16']\n",
      "['Sub-Saharan Africa', 'Ghana', 'Office Supplies', 'Online', 'L', '3/23/2017', '601245963', '4/15/2017', '896', '651.21', '524.96', '583484.16', '470364.16', '113120.00']\n",
      "['Sub-Saharan Africa', 'Tanzania', 'Cosmetics', 'Offline', 'L', '5/23/2016', '739008080', '5/24/2016', '7768', '437.20', '263.33', '3396169.60', '2045547.44', '1350622.16']\n",
      "['Asia', 'Taiwan', 'Fruits', 'Offline', 'M', '2/9/2014', '732588374', '2/23/2014', '8034', '9.33', '6.92', '74957.22', '55595.28', '19361.94']\n",
      "['Middle East and North Africa', 'Algeria', 'Cosmetics', 'Online', 'M', '2/18/2011', '761723172', '2/24/2011', '9669', '437.20', '263.33', '4227286.80', '2546137.77', '1681149.03']\n",
      "['Asia', 'Singapore', 'Snacks', 'Online', 'C', '1/28/2013', '176461303', '2/7/2013', '7676', '152.58', '97.44', '1171204.08', '747949.44', '423254.64']\n",
      "['Australia and Oceania', 'Papua New Guinea', 'Clothes', 'Offline', 'L', '6/20/2011', '647164094', '7/14/2011', '9092', '109.28', '35.84', '993573.76', '325857.28', '667716.48']\n",
      "['Asia', 'Vietnam', 'Personal Care', 'Online', 'M', '4/4/2010', '314505374', '5/6/2010', '7984', '81.73', '56.67', '652532.32', '452453.28', '200079.04']\n",
      "['Sub-Saharan Africa', 'Uganda', 'Personal Care', 'Online', 'M', '6/19/2014', '539471471', '7/21/2014', '451', '81.73', '56.67', '36860.23', '25558.17', '11302.06']\n",
      "['Sub-Saharan Africa', 'Zimbabwe', 'Office Supplies', 'Offline', 'C', '3/28/2011', '953361213', '4/8/2011', '9623', '651.21', '524.96', '6266593.83', '5051690.08', '1214903.75']\n",
      "['Sub-Saharan Africa', 'Ethiopia', 'Cosmetics', 'Online', 'M', '7/7/2011', '807785928', '7/25/2011', '662', '437.20', '263.33', '289426.40', '174324.46', '115101.94']\n",
      "['Europe', 'France', 'Cosmetics', 'Online', 'M', '12/7/2015', '324669444', '1/18/2016', '5758', '437.20', '263.33', '2517397.60', '1516254.14', '1001143.46']\n",
      "['Central America and the Caribbean', 'The Bahamas', 'Personal Care', 'Online', 'C', '1/19/2011', '246248090', '2/21/2011', '9137', '81.73', '56.67', '746767.01', '517793.79', '228973.22']\n",
      "['Central America and the Caribbean', 'Haiti', 'Office Supplies', 'Online', 'C', '12/31/2010', '485070693', '1/31/2011', '2052', '651.21', '524.96', '1336282.92', '1077217.92', '259065.00']\n",
      "['Central America and the Caribbean', 'Nicaragua', 'Household', 'Online', 'C', '10/28/2015', '573998582', '12/7/2015', '7791', '668.27', '502.54', '5206491.57', '3915289.14', '1291202.43']\n",
      "['Asia', 'Turkmenistan', 'Vegetables', 'Online', 'M', '4/13/2015', '116205585', '6/2/2015', '6670', '154.06', '90.93', '1027580.20', '606503.10', '421077.10']\n",
      "['Europe', 'United Kingdom', 'Cosmetics', 'Online', 'L', '5/1/2015', '135178029', '5/16/2015', '1038', '437.20', '263.33', '453813.60', '273336.54', '180477.06']\n",
      "['Central America and the Caribbean', 'Dominican Republic', 'Baby Food', 'Offline', 'H', '8/25/2011', '824714744', '9/24/2011', '274', '255.28', '159.42', '69946.72', '43681.08', '26265.64']\n",
      "['Asia', 'China', 'Office Supplies', 'Online', 'M', '2/10/2016', '198927056', '3/29/2016', '5791', '651.21', '524.96', '3771157.11', '3040043.36', '731113.75']\n",
      "['Sub-Saharan Africa', 'Uganda', 'Cosmetics', 'Online', 'M', '2/28/2015', '842238795', '3/15/2015', '6031', '437.20', '263.33', '2636753.20', '1588143.23', '1048609.97']\n",
      "['Middle East and North Africa', 'Kuwait', 'Household', 'Offline', 'C', '6/13/2011', '459386289', '7/21/2011', '1466', '668.27', '502.54', '979683.82', '736723.64', '242960.18']\n",
      "['Middle East and North Africa', 'United Arab Emirates', 'Office Supplies', 'Online', 'M', '6/23/2012', '425418365', '7/6/2012', '9603', '651.21', '524.96', '6253569.63', '5041190.88', '1212378.75']\n",
      "['Europe', 'Estonia', 'Household', 'Offline', 'H', '9/1/2011', '835696351', '10/21/2011', '9976', '668.27', '502.54', '6666661.52', '5013339.04', '1653322.48']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for row in getstuff(\"../datasets/CSVs/1500000 Sales Records.csv\"):\n",
    "    print(row)\n",
    "    count +=1\n",
    "    if count > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using [HDF5](https://portal.hdfgroup.org/display/HDF5/HDF5)  \n",
    "install: sudo pip3 install h5py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "arr = np.random.randn(1000)\n",
    "\n",
    "#with h5py.File(outputs / 'random.hdf5', 'a') as f:   #if we'd like to append data\n",
    "with h5py.File(outputs / 'random.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"default\", data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets in ths hdf system: default\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "-3.3765271640356636\n",
      "2.7813611665017985\n",
      "[-0.59524694 -1.00467046  1.11106668  0.15856254 -1.18011689  2.11691762\n",
      " -1.28100563  1.66229257  1.46906172  1.03215762  0.59811781 -0.92691749\n",
      " -0.10675675  0.42983497 -0.87583951]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(outputs / 'random.hdf5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "       print(\"datasets in ths hdf system:\", key)\n",
    "    \n",
    "    data = f['default']\n",
    "    print(type(data))\n",
    "    print(min(data))\n",
    "    print(max(data))\n",
    "    print(data[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.random.randn(10000)\n",
    "arr2 = np.random.randn(10000)\n",
    "\n",
    "with h5py.File(outputs / 'complex_read.hdf5', 'w') as f:\n",
    "    f.create_dataset('array_1', data=arr1)\n",
    "    f.create_dataset('array_2', data=arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "[-0.06510376  0.00332522  0.95322672 ...  0.35694943 -1.1481764\n",
      "  0.64560298]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(outputs / 'complex_read.hdf5', 'r') as f:\n",
    "    d1 = f['array_1']\n",
    "    d2 = f['array_2']\n",
    "    print(type(d1))\n",
    "    print(type(d2))\n",
    "    #data = d2[d1>0]  #error\n",
    "    data = d2[d1[:]>0]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dask:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV¶\n",
    "\n",
    "First we read our csv data.\n",
    "\n",
    "CSV and other text-based file formats are the most common storage for data from many sources, because they require minimal pre-processing, can be written line-by-line and are human-readable. Since Pandas' read_csv is well-optimized, CSVs are a reasonable input, but far from optimized, since reading required extensive text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "\n",
    "df_csv = dd.read_csv('../datasets/CSVs/1500000 Sales Records.csv')\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ('../datasets/CSVs/1500000 Sales Records.hdf')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data as before\n",
    "df_hdf = dd.read_hdf(target, '/data')\n",
    "df_hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare CSV to HDF5 speeds\n",
    "\n",
    "We do a simple computation that requires reading a column of our dataset and compare performance between CSV files and our newly created HDF5 file. Which do you expect to be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_csv['Total Profit'].sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_hdf['Total Profit'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly they are about the same, or perhaps even slower.\n",
    "\n",
    "The culprit here are the columns which is of object dtype and thus hard to store efficiently. There are two problems here:\n",
    " - How do we store text data like names efficiently on disk?\n",
    " - Why did we have to read the names column when all we wanted was amount?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
