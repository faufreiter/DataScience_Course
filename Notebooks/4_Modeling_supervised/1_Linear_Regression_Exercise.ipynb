{"cells":[{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (15,4) # the charts will have a size of width = 15, height = 4\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\n#%matplotlib inline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e3e51ef-2a1c-4f9e-a6a0-2bc3ada1124f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Linear regression with 1 feature"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"729188b6-99f6-4c3e-9e7c-87fa33252dfb"}}},{"cell_type":"markdown","source":["## Generate a dataset for Linear Regression\nFor this lesson we will create an artificial dataset using the `sklearn.datasets` module of sklearn."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e522463-5fe6-457c-b70f-ed50248d094c"}}},{"cell_type":"markdown","source":["- With the `make_regression()` function of `datasets` we can generate a synthetical dataset for a regression problem.\n- Here we generate 100 observations with 1 explanatory variable and a standard deviation for the gaussian noise of 40.\n- If you want to read the documentation you can always **run the function name with a questionmark before the name** like in the cell below. This will open the documentation directly in jupyter notebok. You can also read the documentation on the 'internet, e.g. https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14363e39-3220-472f-a61e-95bf9e9627c6"}}},{"cell_type":"code","source":["? datasets.make_regression # read the documentation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c5ba50e-492f-4176-9066-7104f4a0e407"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Object ` datasets.make_regression # read the documentation` not found.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Object ` datasets.make_regression # read the documentation` not found.\n"]}}],"execution_count":0},{"cell_type":"code","source":["x, y, coeff = datasets.make_regression(n_samples = 100, \n                                       n_features = 1,\n                                       noise = 40,\n                                       coef = True,\n                                       bias = 50,\n                                       random_state = 42)\n\n# Funny Note\n# if you have read Hitchiker's Guide to the Galaxy then\n# you know that 42 is the universal answer to life, the universe and everything\n# https://www.quora.com/Why-do-we-choose-random-state-as-42-very-often-during-training-a-machine-learning-model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e825024a-51d5-4a0f-a054-5ed785541f7f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can plot x against y to see what the data look like."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bac653d-637a-42bb-8c1c-4b625cf123e3"}}},{"cell_type":"code","source":["#plt.figure(figsize=(15,4))\nplt.scatter(x,y)\nplt.title('Data')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff9c821f-04ea-4f71-9398-409848e884bd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the user defined function below which plots the observations and a line, and calculates the RMSE. You will use this function in the exercises!\nIt takes as inputs x and y, the intercept value and a coefficient value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d58a5974-2541-4b90-af5b-160de7ac08ce"}}},{"cell_type":"code","source":["def plot_regression(x, y, bias, coeff):\n    \"\"\"\n    The function plots a scatterpot of x, y and a line with bias and coefficient. It also calculates the RMSE.\n    ---------------\n    params:\n    - x: points on the x-axis\n    - y: points on the y-axis\n    - bias: intercept of the line\n    - coeff: slope of the line\n     \"\"\"\n    y_hat = bias + x * coeff # predictions of x can be calculated easily \n                             # by multiplying the features with coefficients\n    print(f'MSE2 : {round(mean_squared_error(y,y_hat),1)}') \n    print(f'RMSE2 : {round(mean_squared_error(y,y_hat,squared=False),1)}')\n\n    # chart\n    plt.title('Observations with a line')\n    plt.scatter(x,y) # scatter\n    plt.plot(x, y_hat, 'r--') # line"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c267e82-0720-4259-9dd9-a7432e0d1b50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise \nWe want to fit a model that looks like this: \n$$\\widehat{y} = \\beta_0 + \\beta_1 x_1 ,$$ \nwhere \\\\(\\beta_0\\\\) is a bias term and \\\\(\\beta_1\\\\) is the slope of the line.\n\nUse our user-defined function `plot_regression()` and try different values of bias and coeff of the regression line. Observe the change in the values of the cost function and the behavior of the line with the change of the parameters. \n- What does a line look like if the coeff is positive, negative or zero?\n- What is the influence of the bias term on the line?\n- Can you guess a suitable set of parameters? \n- What are the units of the MSE and RMSE in relation to dependent variable y? Which one is more intuitive to use for interpretation?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1f5f207-445b-402c-ac60-ed2cbc5f88e7"}}},{"cell_type":"code","source":["# Task: Try different values of bias and coeffs\n\n# plot_regression(x, y, bias=..., coeff=...)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c9f42da-127f-49cf-878f-ce8c8455b5b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Normal Equation\n\nThe function `normal_eq()`  computes \\\\(\\widehat{\\beta}= (X^T X)^{-1} X^T y \\\\). It takes as input X and y and returns optimal values for the bias (intercept) term and the coefficient (slope)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6832749-8ed0-437a-bffa-24882f574a41"}}},{"cell_type":"code","source":["def normal_eq(x,y): \n    \"\"\"\n    The function analytically computes the optimal set of coefficients given x and y. \n    A vector of ones is appended as the first vector of matrix x to take into account the bias term. \n    ---------------\n    params:\n    - x: input features matrix\n    - y: target variable\n    returns: \n    - beta_hat: optimal set of coefficients for linear regression\n    \"\"\"\n    X = np.c_[np.ones(len(x)),x]\n    beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    print(f\"Optimal set of coefficients: {beta_hat}\")\n    return(beta_hat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a689251-05a1-4c61-9543-7f9b01d6220d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise\n\n- Use our user-defined function `normal_eq()` on the input features x and the output vector y.\n- Use the returned values of the bias and coef in the `plot_regression` function.\n\nWas your guess for the bias and coeff value from the previous exercise close enough?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58588e20-e566-497c-98c7-a80760779998"}}},{"cell_type":"code","source":["# Task: use normal_eq() and find coeffs\n\n#..., ... = normal_eq(x, y) \n#plot_regression(..., ..., ..., ...)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78f9bb37-4466-42bd-ac67-abe7e51f4cbc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Sklearn Linear Regression\n\nHere we explore the linear regression from scikit learn for the first time. Help yourself with [examples from the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) if needed: \n\n### Exercise\n- Use `LinearRegression()` from `sklearn.linear_model` to fit the linear regression model on x, y. Look at the examples section in the documentation if you need help.\n- Return the coefficients for slope and intercept of the regression line. You can find them in the attributes section of the documentation. Store the values in the variables `lr_coef` and `lr_intercept`.\nAre these values the same as the ones from the normal equation?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"711a9ed3-5631-49af-99b2-30ecffef7341"}}},{"cell_type":"code","source":["# Task: implement linear regression\n\nlr = ...\nlr.fit(..., ...)\nlr_coef, lr_intercept = lr...., lr....\nprint(f'Slope: {lr_coef}\\nBias: {lr_intercept}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57375b0f-85b7-4ddc-9647-60aa212001b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- Next, predict the value of the new observation. If needed use the documentation for some examples."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb4e330f-4d5e-4b6f-8216-7a70498f105d"}}},{"cell_type":"code","source":["# Task: predict the value of new observations\n\nx_new = [[1.5], [0]]\nlr.predict(...)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6518947d-b59d-4f65-8cc6-372c11d58ead"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- Lastly, return the score of the model on x and y. You can read more about the score in the documentation. The best value is 1. Usually it is between 0 and 1 but it can be also negative. The score is the R-squared metric that can be used for the evaluation of the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0663987c-939d-4eb8-8c00-ab8bf53562f9"}}},{"cell_type":"code","source":["# Task: predict the value of new observations\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a97087ce-a7d7-4501-bfc1-10ce61effec7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Outliers\n\nWe will now add outliers to our dataset and save them under the names x2 and y2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80c5f1a6-6ba3-492a-8aee-73399d8479a3"}}},{"cell_type":"code","source":["x2 = np.append(x,[np.min(x)-0.1, np.min(x), np.min(x)-0.15]).reshape([-1,1])\ny2 = np.append(y, [-400,-300,-350]).reshape([-1,1])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4579d6e9-f626-4288-bb08-20562270aaba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise\n\nFit the linear regression to x2, y2 and store the bias in the variable `lr_outlier_intercept` and the slope in the variable `lr_outlier_coef`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"095fed39-8c73-487a-aa8f-2a0a0c81d71e"}}},{"cell_type":"code","source":["# Task: Fit linear regression\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d444b0a-ef5d-4edf-8291-17703cf36a18"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can observe on the chart how outliers influence the regression line. Outlier treatment should be one of the first steps done before fitting a linear regression model, otherwise the results can be biased."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a15a185-4ee1-4d1b-9f37-937650081ab7"}}},{"cell_type":"code","source":["plt.scatter(x2,y2)\naxes = plt.gca()\nx_vals2 = np.array(axes.get_xlim())\ny_vals = lr_intercept + lr_coef * x_vals2\ny_vals2 = lr_outlier_intercept.reshape([1,]) + lr_outlier_coef.reshape([1,]) * x_vals2\nplt.plot(x_vals2, y_vals, 'r--', label='original regression line')\nplt.plot(x_vals2, y_vals2, 'b--', label='regression line with outliers')\nplt.legend()\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b5e1c5d-9bed-40ee-967b-55bb63e333fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Multiple Linear Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b41585c-0be0-4688-9a5e-3ccfa3974375"}}},{"cell_type":"markdown","source":["## Load Dataset\n\nLoad sklearn's inbuilt dataset for regression. If you want you can read the description of the dataset [here](http://lib.stat.cmu.edu/datasets/boston). It has 13 attributes that can be used for predicting house prices."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54dd6edf-83e0-4d8e-8bb5-5427cbc95a13"}}},{"cell_type":"code","source":["raw_df = pd.read_csv('Data/Boston.csv')\n\ny = pd.DataFrame(raw_df['target'])\nx = pd.DataFrame(raw_df.iloc[:,1:-1])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22b0b59d-534a-4c6e-84fc-e71a750d5488"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Train Test Split\n\n### Exercise \nUse the function `train_test_split()` to split the training data into training and testing datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7b1ecd3-c007-41d9-99b7-b700ddd54ae5"}}},{"cell_type":"code","source":["# Task: use function train_test_split() to split the training data into training and testing dataset\n\nX_train, X_test, y_train, y_test = train_test_split(..., ..., random_state=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a6a28f0-db8b-4128-996d-55d7e6d70281"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Fit the Model\n### Exercise\n- Instantiate the linear regression model.\n- Fit the model to the training data.\n- Print the value of the intercept of the model.\n- Return the values of the coefficients and save them under the variable `model_coef`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cb9e7cf-e71e-440f-a730-a26ad95e220d"}}},{"cell_type":"code","source":["# Task: instantiate the model\n\n\n# Task: fit the model to x, y\n\n\nprint(f\"intercept: {...}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6101c336-bdeb-46c0-8c96-fc0916fdf865"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can interpret whether the given feature influences the prediction negatively or positively based on the sign of the coefficient. Also, if all the other variables are unchanged we can see how this single variable affects the output by changing it by 1 unit."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a523e5a-1974-4f1b-b871-990141114338"}}},{"cell_type":"code","source":["df_coefs = pd.DataFrame(model_coef, index = [\"Coefficient\"], columns=X_train.columns)\ndisplay(df_coefs)\nprint(f'''For example, if you have an observation where LSTAT is equal to 50 and another one that has a value \nof LSTAT 51 and all other variables are the same, or if the variable LSTAT is changed for the investigated observation \nwith other variables unchanged the effect on the target would be {np.round(df_coefs['LSTAT'][0],5)}''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72e00c48-5894-4f72-87c8-a2b17eb48a93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Prediction / Model Evaluation\n### Exercise\n\nPredict on `X_test` and store the values in `y_hat`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f193795b-0cb0-4275-850a-01495497bfc2"}}},{"cell_type":"code","source":["# Task: predict on X_test and store the values into y_hat\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f51697f6-d386-43a0-82ff-ae65629ce021"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below is the plot of predictions against real values. Most of the data points lie around a diagonal line. This means that the predictions seem to be in line with the actual values, e.g. if the real value was 20, the prediction is also almost 20."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f24aec6d-baa3-45c7-a98b-ec6588e1b89d"}}},{"cell_type":"code","source":["plt.scatter(y_hat, y_test)\nplt.plot([0,50],[0,50],c='b')\nplt.xlabel(\"predictions\")\nplt.ylabel(\"true test values\")\nplt.show();"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"948c8385-7776-402a-8f22-126bd3a4c3c4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise\n\nLet's have a look at some metrics. Compute and save the following metrics on the test set:\n\n- MSE -  `mean_squared_error` from `sklearn.metrics` (see ?mean_squared_error)\n- RMSE - `mean_squared_error`  from `sklearn.metrics` with the parameter `squared` set to `False`\n- MAE (Mean absolute error) - `mean_absolute_error`  from `sklearn.metrics` (see ?mean_absolute_error)\n- R2 (score) - this is an attribute of `LinearRegression()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ccf3a8b-b24b-42be-93f2-d8bec9282674"}}},{"cell_type":"code","source":["# Task: Compute and save the metrics on the test set\n\nmse = mean_squared_error(y_test, y_hat)\nrmse = mean_squared_error(..., ..., ...)\nmae = ...(..., ...)\nr2 = model.score(..., ...) # the same as r2_score(y_test, y_hat)\n\nprint(f\"MSE: {np.round(mse, 1)}\")\nprint(f\"RMSE: {np.round(rmse, 1)}\")\nprint(f\"MAE: {np.round(mae, 1)}\")\nprint(f\"R2: {np.round(r2, 1)}\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13c54195-910c-4911-8971-66e83092a3e7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Scaling\n\n### Exercise\nIf features are not scaled appropriately, the intercept tells you what the expected value for the target variable would be if all the variables were equal to 0. This might be unrealistic for many features such as weight, size of the house, distance to the sea, etc. \nWhen features are scaled correctly, the intercept can be interpreted as the expected value of a target variable when all the features are equal to their averages.\n\nFor the next exercise you will scale your features using `StandardScaler()`.\n- Instantiate `StandardScaler()` from `sklearn.preprocessing`.\n- Fit the scaler to `X_train` and transform it. Save the transformed values into `X_train_scaled`.\n- Transform the `X_test` data with the fitted scaler and save the transformed values into `X_test_scaled`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"600f0f67-9b7f-423c-8fd9-f10e726acf62"}}},{"cell_type":"code","source":["# Task: Scaling\n\nscaler = ...\nX_train_scaled = scaler.fit_transform(...)\nX_test_scaled = scaler.transform(...)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b51b7015-2469-444f-a60a-165350bc5641"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If you scaled properly you will see in the boxplot chart on the right that the distribution of the variables are concentrated around zero and that the variance is similar for all the variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4df59228-886c-4a3b-b6a9-3f0ca9b722ec"}}},{"cell_type":"code","source":["plt.subplot(1,2,1)\nplt.title('Boxplots for original features')\nsns.boxplot(data=X_train)\nplt.subplot(1,2,2)\nplt.title('Boxplots for scaled features')\nsns.boxplot(data=X_train_scaled);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13f4f45e-336d-44d4-9b61-d558c199eedb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise\n\n- Fit a linear regression model to `X_train_scaled`, `y_train`.\n- Return the value for the intercept.\n- Return the values for the coefficients and save them to the variable `model_coef`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8121697-e7e0-4c49-b8ae-d141786566b0"}}},{"cell_type":"code","source":["# Task: Fit a linear regression, return values and save them to the variable model_coef\n\n...\n...\nprint(f\"intercept: {...}\")\n..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e664d97-0377-4115-b531-764695ca69fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_coefs = pd.DataFrame(model.coef_.T, index = X_train.columns, columns=[\"Coefficient\"]) \\\n            .sort_values(\"Coefficient\") \\\n            .T\ndisplay(df_coefs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8819016-ff5f-4ad7-af42-c8871dd97afc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["After applying `StandardScaler()` it is easier to compare coefficients with each other as the features all have the same scale. In our example the positive effect of the 'RM' variable on the output is a little bit smaller than the negative effect of the 'LSTAT' variable. In that vein, we can order the coefficients by their absolute magnitudes to understand the influence of the variables on the result. \n\nWe could then try to fit the model with the variables that have the highest coefficient values in absolute terms."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61ecb5e5-5dc2-4580-ade8-0d4d2121f8cf"}}},{"cell_type":"markdown","source":["### Exercise \n\n- Predict with the fitted model on `X_test_scaled`.\n- Calculate MSE, RMSE, MASE and R2. Did it change compared to the unscaled version of linear regression?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd74e46d-89f3-41dc-9fcf-535f3de716af"}}},{"cell_type":"code","source":["# Task\n\n...\nmse = ...\nrmse = ...\nmae = ...\nr2_score = ...\n\nprint(f\"MSE: {np.round(mse,1)}\")\nprint(f\"RMSE: {np.round(rmse,1)}\")\nprint(f\"MAE: {np.round(mae,1)}\")\nprint(f\"R2: {np.round(r2_score,1)}\") # the same as r2_score(y_test, y_hat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e9959be-39d6-4dd6-9213-813aa6b492b3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The scaling **does not have an effect on the performance of the model**. It only helps with interpretability of the coefficients and changes the meaning of the intercept."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be22a36e-5c75-47d9-b2cb-0e0f13da48e1"}}},{"cell_type":"markdown","source":["## Result Analysis\nNow that we have fitted a model on the standardized features and calculated the different scores, we need to analyse these results.\n\n### Residuals\n### Exercise\n- Calculate residuals by deducting `y_test` from `y_hat`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"342d25f4-6be1-47c0-8295-bdc3c0f26a77"}}},{"cell_type":"code","source":["# Task: Calculate residuals by deducting y_test from y_hat\n\nresiduals = ..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69ddfc9c-c427-4913-ad54-69eab4452456"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One of the assumptions of a linear regression is that the **residuals are normally distributed**. \n\nFrom the histogram below it seems that the residuals are almost normally distributed. If this is actually the case can be tested with e.g. the Kolmogorov-Smironov test or the Shapiro-Wilk test. It is also possible to draw a quantile-quantile plot. We could investigate outliers to check if they impact the residuals."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bfcccbe-c6b1-4938-a473-4a2113dbf1a3"}}},{"cell_type":"code","source":["plt.subplot(1,2,1)\nplt.scatter(y_test, residuals)\nplt.ylabel(\"residuals\")\nplt.xlabel(\"y_test\")\nplt.title(\"Residuals against true values\")\n\nplt.subplot(1,2,2)\nplt.hist(residuals)\nplt.xlabel(\"residuals\")\nplt.ylabel(\"frequency\")\nplt.title(\"Residuals histogram\")\nplt.show();"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5977044-2c1a-4549-b4c5-348d107932a9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise – The Dependent Variable\n\nThe lower long tail on the histogram might be also due to the distribution of the dependent variable y. When doing exploratory data analysis we should look only at the train set so that we do not detect patterns from the test set. The test set is set aside and is used only for evaluation.\n\n- Create a boxplot of `y_train`.\n- Plot a histogram of `y_train`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20cbbda5-569d-49a7-ae40-cc7e99433b7b"}}},{"cell_type":"code","source":["# Task - create a boxplot\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a275ead3-5157-444b-9f16-b2198cc0d628"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It seems that there are some outliers on the upper part. This is expected as there can be some very expensive houses."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88be777b-eab6-4329-b102-00dac66319ae"}}},{"cell_type":"code","source":["# Task - create a histogram\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99230099-0962-42da-896b-64ba3963abd9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The distribution of the target variable seems to be rather bimodal. Linear regression does not perform the best for such distributions. However, if we did not take into consideration the outliers, the distribution would be closer to a normal distribution. However, are they really outliers? \nSo the question is if we should **transform the dependent variable**. We could also consider creating two separate linear models, one for the usual values and one for the values on the upper end."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b515c56c-cd3b-4ebb-8e41-cd9253c35876"}}},{"cell_type":"markdown","source":["### Exercise – Outliers\n\nMight the skewness of the residuals be due to the outliers in the explanatory features?\n\n- Draw a boxplot for `X_train`. You can use for example a boxplot from the seaborn library."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5eed84f-e026-493b-9f45-ac952cf56c77"}}},{"cell_type":"code","source":["# Task\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23f62677-1a67-4a2d-9ada-fdd7674d0c9a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There seem to be quite some outliers, e.g. in CRIM, ZN and PTRATIO B. These outliers should be treated."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36b4b501-7982-4c98-ba31-89874ef96a8e"}}},{"cell_type":"markdown","source":["### Exercise – Multicollinearity\n\nCorrelated features might cause the model to be quite unstable. \n\n- Calculate the correlations between all the variables with the `corr()` method called on `X_train`. Save the output into variable `corr`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3154b100-b459-452e-9bfb-abc03778fbfa"}}},{"cell_type":"code","source":["# Task\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"330bce5d-0f12-443c-94bc-20fa1c861403"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below you can see correlations of the variables between each other on the left chart. Right chart shows absolute values of correlations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a0816e-642f-4199-a724-7a7f0b9f02bb"}}},{"cell_type":"code","source":["# charts for correlation\nplt.subplot(1,2,1)\nplt.title('Correlations')\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask = mask, annot=True, cmap='Reds');\n\nplt.title('Absolute value of correlations')\nplt.subplot(1,2,2)\ncorr_abs = corr.abs()\nsns.heatmap(corr_abs, mask = mask, annot=True, cmap='Reds');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9834f876-c1a6-477a-97c8-0fa8be2d1c11"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There is a strong correlation between some features, e.g. TAX and RAD, DIS and NOX, DIS and INUDS, DIS and AGE, LSTAT and RM. **Highly correlated features in linear models can cause instability of the model**. Thus it should be tested how the models perform if we remove such features. Usually one of the features with a correlation higher than some threshold is removed from the feature set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0b4e22a-d27e-4f7f-ac8b-72b42d941d99"}}},{"cell_type":"markdown","source":["### Linearity – Exercise\n\nSince we work with a linear model, there should be a linear relationship between X and y. Let's do some simple checks. \n\n- Compute the correlation between `X_train` and `y_train`. Hint: you can concatenate `X_train` and `y_train` first on `axis=1` and calculate the correlation on the concatenated data frame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"980e6ef8-cb1a-4924-aa8e-5c7780cd4222"}}},{"cell_type":"code","source":["# Task\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d92f083-0212-4160-af04-3711d500b315"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercise\n\nThere is a strong negative correlation between the target variable and the 'LSTAT' feature, as well as a strong positive correlation between the target variable and the 'RM' feature. \n\n- Draw a scatter plot showing the dependency between X and y not only for the two most correlated features 'LSTAT' and 'RM' but also for the other features to investigate if there is any pattern."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8f2974b-f3d3-4035-acae-24d70557de86"}}},{"cell_type":"code","source":["# Task\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae7cff23-2c73-43ad-8c6c-c0bd4447886e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We already know that there is a strong correlation between the target and the variable 'LSTAT' but the relationship seems to be rather non-linear. We can transform the variable 'LSTAT' for example with a negative logarithm or x-squared function to get a linear relationship between the modified variable and y. \n\nThe relationship with the second highest correlated feature 'RM' seems to be linear."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8cb6553-1131-4893-a7bc-b7d801b4d96a"}}},{"cell_type":"markdown","source":["## Model only with the 2 strongest variables \n### Exercise\n- Fit the model only with the features 'LSTAT' and 'RM'.\n- Predict and return the RMSE."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e286ed2d-0846-4f4f-8726-76203f3c98f4"}}},{"cell_type":"code","source":["# Task\n\n...\nrmse = \nprint(f\"RMSE: {np.round(rmse,1)}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3abf9580-60cc-4c4d-8172-625325981e83"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This model has a slightly worse performance than the original model with an RMSE of around 4.7. However, interpretaion of the model with 2 variables is easier and usually the generalization of the simpler model is also better."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dc85ede-f2b0-445a-907e-50fe029e0709"}}},{"cell_type":"markdown","source":["## 2nd degree polynomial features\n\nWe have seen in the scatter plots that there likely is a non-linear relationship between 'LSTAT' and the target variable. It makes sense to test the model with the 2 variables as above but also adding polynomial features of the 2nd degree and an interaction term between the two variables. Run the cell below to see the performance of such a model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e965b78-6572-4056-9cef-556bc2c8b30d"}}},{"cell_type":"code","source":["poly = PolynomialFeatures(2)\nx_train_poly = poly.fit_transform(X_train[['LSTAT', 'RM']])\nx_test_poly = poly.transform(X_test[['LSTAT', 'RM']])\n  \nmodel_poly = LinearRegression(fit_intercept=False)\nmodel_poly.fit(x_train_poly, y_train)\n    \ny_hat = model_poly.predict(x_test_poly)\nprint(f\"RMSE: {mean_squared_error(y_test, y_hat, squared=False)}\")\npd.DataFrame(model_poly.coef_, index = [\"Coefficient\"], columns = poly.get_feature_names_out())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9df7a7f9-85e6-4ada-87bf-65771a89d23f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This model has so far the best performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80be8436-6cfd-4137-8db3-92359f9f69a4"}}},{"cell_type":"markdown","source":["## Next steps to consider\nHow can we improve our current model even further?\n\n- Treatment of the y variable - Should we use two different models? Should we transform y?\n- Outliers need to be treated.\n- Feature selection methods.\n- Removal of correlated features.\n- Feature engineering.\n- ..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34bf6927-0670-495a-90a2-42f5c0e814dd"}}},{"cell_type":"markdown","source":["------------------------------------------------------------------------------------------------------------\nMaterial adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06e72d80-350a-4e32-86d3-549319ec3e3d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1_Linear_Regression_Exercise","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2647897086957784}},"nbformat":4,"nbformat_minor":0}
