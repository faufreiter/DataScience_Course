{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cd7c63d-2c10-42f0-abad-4f101708548b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## AutoML Basic Concepts\n",
    "\n",
    "[Source](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea6139cb-70df-4d7d-8351-d033cdf0fb23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3ca373f-60dc-48cc-b49a-e8bc611a4802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b03cd05-c08d-4409-84b9-18817934a567",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "titanic = pd.read_csv('data_titanic/train.csv')\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a902b222-d3fc-4a21-b404-2933ec9fe2f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Exploration\n",
    "Before we get going with TPOT, we start with some simple data exploration to understand our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1193ce34-e905-40af-b24d-4eef6576ffdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic.groupby('Sex').Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "688c0d66-16f5-4b66-ad63-a2e7ecb7c82f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['Pclass','Sex']).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6149a782-5957-4e7c-80c9-ea5ae9d04117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "id = pd.crosstab([titanic.Pclass, titanic.Sex], titanic.Survived.astype(float))\n",
    "id.div(id.sum(1).astype(float), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5688ac28-41b7-4fd1-bd03-f66d3db0445e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Munging\n",
    "The first and most important step in using TPOT on any data set is to rename the target class/response variable to 'class'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1faff52-3e00-4022-ad12-5c3ff2051053",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic.rename(columns={'Survived': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84e4d53d-2588-4d6f-a07e-e1a04c86306c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "At present, TPOT requires all the data to be in numerical format. As we can see below, our data set has 5 categorical variables which contain non-numerical values: 'Name', 'Sex', 'Ticket', 'Cabin' and 'Embarked'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f3f4941-d587-4f53-b94f-67bfc2038aef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94eea970-6403-4117-b476-3a1131d79d03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We then check the number of distinct levels that each of the five categorical variables have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b93c5ba0-cb8e-4385-befa-d906a233e10c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n",
    "    print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(cat, titanic[cat].unique().size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccb99d9e-439e-4ae2-8451-c69684639b70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, 'Sex' and 'Embarked' have only very few levels. Let's find out what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c64aebb-5f31-403b-8185-6b6c8abb5e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for cat in ['Sex', 'Embarked']:\n",
    "    print(\"Levels for catgeory '{0}': {1}\".format(cat, titanic[cat].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "90b0be2a-11af-4e05-a3a6-69b9fef36224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We then code these levels manually into numerical values. For NaN i.e. the missing values, we simply replace them with a placeholder value (-999). In fact, we perform this replacement for the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f34981bc-a631-4e7e-8faf-c7992b8ade4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})\n",
    "titanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5232da02-c588-484c-85a8-7d94eca48569",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic = titanic.fillna(-999)\n",
    "pd.isnull(titanic).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51f68dd7-1d1b-4590-845b-49521317b9d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since 'Name' and 'Ticket' have so many different levels, we drop them in this example from our analysis for the sake of simplicity. For 'Cabin', we encode the levels as digits using Scikit-learn's `MultiLabelBinarizer` and treat them as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7aafee80-cc0b-4c69-89a2-a0793be91a99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "CabinTrans = mlb.fit_transform([{str(val)} for val in titanic['Cabin'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9277e16a-fa85-4b77-bcad-7dc425b0f000",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CabinTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c6cb9e0-5829-4731-8192-70a4b1796588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Drop the unused features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d605257-ae04-4b54-b868-de15eca4d191",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_new = titanic.drop(['Name','Ticket','Cabin','class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3dda327-ad55-4ef6-b585-890ce60e5726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert (len(titanic['Cabin'].unique()) == len(mlb.classes_)), \"Not Equal\" #check correct encoding done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "533c38d3-b670-4e1f-a181-1e4d0ad0af22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We then add the encoded features to form the final dataset to be used with TPOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "56c3ecf4-22b3-4ac2-b271-455d3f5337f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_new = np.hstack((titanic_new.values,CabinTrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e70ae3ed-76ed-4550-87df-0709343982f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.isnan(titanic_new).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf84f9e2-07fd-421b-922d-d8cae5f27704",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Keep in mind that the final data set is in the form of a numpy array. We can check the number of features in the final data set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e2f7469-57a7-4dd9-a545-825085aba506",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_new[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dfd6e32f-911b-434d-9fe8-1df18348364e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we store the class labels which we need to predicted in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fca94ec9-cab4-4ae7-839d-378ffc98f055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_class = titanic['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0dbb98c2-e769-456a-bbc7-6e0a33c61b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Analysis using TPOT\n",
    "To begin our analysis, we need to divide our training data into training and validation sets. The validation set is just to give us an idea of the test set error. The model selection and tuning is entirely taken care of by TPOT, so if we want to, we can skip the creation of this validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbcf3486-a736-427e-9251-72e476967a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_indices, validation_indices = training_indices, testing_indices = train_test_split(titanic.index, stratify = titanic_class, train_size=0.75, test_size=0.25)\n",
    "training_indices.size, validation_indices.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c775ae6-0669-4e7f-9a6b-df82f490b320",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After that, we proceed with calling the fit, score and export functions on our training dataset. To get a better idea of how these functions work, refer to the TPOT documentation here.\n",
    "\n",
    "An important TPOT parameter to set is the **number of generations**. Since our aim is to just illustrate the use of TPOT, we set maximum optimization time to 2 minutes (`max_time_mins=2`). On a standard laptop with 4GB RAM it takes roughly 5 minutes per generation to run. For each added generation, it should take 5 minutes more. Thus, for the default value of 100, the total run time could be roughly around 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55a96f42-834e-4b91-af01-8ca0f4de02f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(disable=True)  # As MLFlow is not totally integrated with TPOT, we disable the autologging when running on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b659f08-9164-499a-be66-663333a2997a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(verbosity=2, max_time_mins=2, max_eval_time_mins=0.04, population_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55934afd-078c-4215-92cc-ca734952ddba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot.fit(titanic_new[training_indices], titanic_class[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72188cfc-ea29-466a-9b4b-22d2f0ca0e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot.score(titanic_new[validation_indices], titanic.loc[validation_indices, 'class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0b9e2e4-dbcd-497d-b4ea-1be3f585110e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tpot.export('/tmp/tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df161a0f-d81c-4fe1-b41b-0979225e6a40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's have a look at the generated code. As we can see, the random forest classifier performed the best on the given dataset out of all the other models that TPOT currently evaluates on. If we ran TPOT for more generations, then the score should improve further.\n",
    "\n",
    "Run:\n",
    "> `%load tpot_titanic_pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd8d1e12-e042-4714-86c7-76d29215a43d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%load /tmp/tpot_titanic_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: 0.8308382897542362\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.8500000000000001, min_samples_leaf=18, min_samples_split=14, n_estimators=100)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "209d3e08-b42c-4db4-8796-8b7086ca1d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Make predictions on the submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0826a1d1-cb64-46b2-bddb-e9e7fd9e80a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in the submission dataset\n",
    "titanic_sub = pd.read_csv('data_titanic/test.csv')\n",
    "titanic_sub.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6eae42c2-26a2-4c00-860d-1a693bd60cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When looking at fresh data to make predictions on, the most important step is to **check for new levels in the categorical variables** of the submission data set which were absent in the training set. We identify them and set them to our placeholder value of `-999`, i.e., we treat them as missing values. This ensures training consistency, as otherwise the model would not know what to do with the new levels in the submission data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74861ad1-4867-4549-a56f-b72ac3825adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for var in ['Cabin']: #,'Name','Ticket']:\n",
    "    new = list(set(titanic_sub[var]) - set(titanic[var]))\n",
    "    titanic_sub.loc[titanic_sub[var].isin(new), var] = -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d75d40a-ed87-415e-b064-44cda616d4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We then carry out the data munging steps as done earlier for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e28f3bd7-c27e-4ba5-814b-00dd216ff7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_sub['Sex'] = titanic_sub['Sex'].map({'male':0,'female':1})\n",
    "titanic_sub['Embarked'] = titanic_sub['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5d0b27ee-d901-4079-990d-a52b2e148f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_sub = titanic_sub.fillna(-999)\n",
    "pd.isnull(titanic_sub).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96f6d6fe-9508-467a-95de-e13cf7fd8c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "While calling `MultiLabelBinarizer` on the submission data set, we first fit on the training set again to learn the levels and then transform the submission data set values. This further ensures that only those levels that were present in the training data set are transformed. If new levels are still found in the submission data set then it will return an error and we need to go back and check our earlier step of replacing new levels with the placeholder value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba6366b1-3a16-477e-8eb4-d7d83ba61550",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "SubCabinTrans = mlb.fit([{str(val)} for val in titanic['Cabin'].values]).transform([{str(val)} for val in titanic_sub['Cabin'].values])\n",
    "titanic_sub = titanic_sub.drop(['Name','Ticket','Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d4f67f5-54de-48bf-9914-bd2c119265fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Form the new submission data set\n",
    "titanic_sub_new = np.hstack((titanic_sub.values,SubCabinTrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a487551-5ad8-4b74-a99c-abfffc9e9bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.any(np.isnan(titanic_sub_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a55f9ef9-9f01-4802-a420-ed6c875e42fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure an equal number of features in both the final training and submission dataset\n",
    "assert (titanic_new.shape[1] == titanic_sub_new.shape[1]), \"Not Equal\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13ec12a1-22ab-4150-8f00-b421f7ebfa0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate the predictions\n",
    "submission = tpot.predict(titanic_sub_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "847eed60-1b3a-4104-b6aa-28d84e9e7a8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the submission file\n",
    "final = pd.DataFrame({'PassengerId': titanic_sub['PassengerId'], 'Survived': submission})\n",
    "#final.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25f3b824-c8e6-46c2-ab91-0e9194977925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3dc3857c-59e7-4dc6-ad1b-89d16e1d3016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There we go! We have successfully generated the predictions for the 418 data points in the submission dataset, and we're good to go ahead to submit these predictions on Kaggle."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3_AutoML_example",
   "notebookOrigID": 2649658360540072,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
