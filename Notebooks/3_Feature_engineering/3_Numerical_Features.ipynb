{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library \n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import MinMaxScaler to scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>Small Hass Avocado</th>\n",
       "      <th>Large Hass Avocado</th>\n",
       "      <th>Extra Large Hass Avocado</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.75</td>\n",
       "      <td>27365.89</td>\n",
       "      <td>9307.34</td>\n",
       "      <td>3844.81</td>\n",
       "      <td>615.28</td>\n",
       "      <td>13598.46</td>\n",
       "      <td>13061.10</td>\n",
       "      <td>537.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.49</td>\n",
       "      <td>17723.17</td>\n",
       "      <td>1189.35</td>\n",
       "      <td>15628.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>905.55</td>\n",
       "      <td>905.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2896.72</td>\n",
       "      <td>161.68</td>\n",
       "      <td>206.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2528.08</td>\n",
       "      <td>2528.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>HarrisburgScranton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.52</td>\n",
       "      <td>54956.80</td>\n",
       "      <td>3013.04</td>\n",
       "      <td>35456.88</td>\n",
       "      <td>1561.70</td>\n",
       "      <td>14925.18</td>\n",
       "      <td>11264.80</td>\n",
       "      <td>3660.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Pittsburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1505.12</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1129.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>374.35</td>\n",
       "      <td>186.67</td>\n",
       "      <td>187.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Boise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume  Small Hass Avocado  \\\n",
       "0  2015-01-04          1.75      27365.89             9307.34   \n",
       "1  2015-01-04          1.49      17723.17             1189.35   \n",
       "2  2015-01-04          1.68       2896.72              161.68   \n",
       "3  2015-01-04          1.52      54956.80             3013.04   \n",
       "4  2015-01-04          1.64       1505.12                1.27   \n",
       "\n",
       "   Large Hass Avocado  Extra Large Hass Avocado  Total Bags  Small Bags  \\\n",
       "0             3844.81                    615.28    13598.46    13061.10   \n",
       "1            15628.27                      0.00      905.55      905.55   \n",
       "2              206.96                      0.00     2528.08     2528.08   \n",
       "3            35456.88                   1561.70    14925.18    11264.80   \n",
       "4             1129.50                      0.00      374.35      186.67   \n",
       "\n",
       "   Large Bags  XLarge Bags          type  year              region  \n",
       "0      537.36          0.0       organic  2015           Southeast  \n",
       "1        0.00          0.0       organic  2015             Chicago  \n",
       "2        0.00          0.0       organic  2015  HarrisburgScranton  \n",
       "3     3660.38          0.0  conventional  2015          Pittsburgh  \n",
       "4      187.68          0.0       organic  2015               Boise  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load avocado dataset and store it in the variable dataframe\n",
    "dataframe = pd.read_csv('Data/avocado.csv')\n",
    "# Get the first 5 rows\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18249 entries, 0 to 18248\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Date                      18249 non-null  object \n",
      " 1   AveragePrice              18249 non-null  float64\n",
      " 2   Total Volume              18249 non-null  float64\n",
      " 3   Small Hass Avocado        18249 non-null  float64\n",
      " 4   Large Hass Avocado        18249 non-null  float64\n",
      " 5   Extra Large Hass Avocado  18249 non-null  float64\n",
      " 6   Total Bags                18249 non-null  float64\n",
      " 7   Small Bags                18249 non-null  float64\n",
      " 8   Large Bags                18249 non-null  float64\n",
      " 9   XLarge Bags               18249 non-null  float64\n",
      " 10  type                      18249 non-null  object \n",
      " 11  year                      18249 non-null  int64  \n",
      " 12  region                    18249 non-null  object \n",
      "dtypes: float64(9), int64(1), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the dataframe\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numerical variables to create DataFrame data\n",
    "data = dataframe.select_dtypes(include = ['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variable scale / Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>Small Hass Avocado</th>\n",
       "      <th>Large Hass Avocado</th>\n",
       "      <th>Extra Large Hass Avocado</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18249.000000</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>18249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.405978</td>\n",
       "      <td>8.506440e+05</td>\n",
       "      <td>2.930084e+05</td>\n",
       "      <td>2.951546e+05</td>\n",
       "      <td>2.283974e+04</td>\n",
       "      <td>2.396392e+05</td>\n",
       "      <td>1.821947e+05</td>\n",
       "      <td>5.433809e+04</td>\n",
       "      <td>3106.426507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.402677</td>\n",
       "      <td>3.453545e+06</td>\n",
       "      <td>1.264989e+06</td>\n",
       "      <td>1.204120e+06</td>\n",
       "      <td>1.074641e+05</td>\n",
       "      <td>9.862424e+05</td>\n",
       "      <td>7.461785e+05</td>\n",
       "      <td>2.439660e+05</td>\n",
       "      <td>17692.894652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>8.456000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.083858e+04</td>\n",
       "      <td>8.540700e+02</td>\n",
       "      <td>3.008780e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.088640e+03</td>\n",
       "      <td>2.849420e+03</td>\n",
       "      <td>1.274700e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.370000</td>\n",
       "      <td>1.073768e+05</td>\n",
       "      <td>8.645300e+03</td>\n",
       "      <td>2.906102e+04</td>\n",
       "      <td>1.849900e+02</td>\n",
       "      <td>3.974383e+04</td>\n",
       "      <td>2.636282e+04</td>\n",
       "      <td>2.647710e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.660000</td>\n",
       "      <td>4.329623e+05</td>\n",
       "      <td>1.110202e+05</td>\n",
       "      <td>1.502069e+05</td>\n",
       "      <td>6.243420e+03</td>\n",
       "      <td>1.107834e+05</td>\n",
       "      <td>8.333767e+04</td>\n",
       "      <td>2.202925e+04</td>\n",
       "      <td>132.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.250000</td>\n",
       "      <td>6.250565e+07</td>\n",
       "      <td>2.274362e+07</td>\n",
       "      <td>2.047057e+07</td>\n",
       "      <td>2.546439e+06</td>\n",
       "      <td>1.937313e+07</td>\n",
       "      <td>1.338459e+07</td>\n",
       "      <td>5.719097e+06</td>\n",
       "      <td>551693.650000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AveragePrice  Total Volume  Small Hass Avocado  Large Hass Avocado  \\\n",
       "count  18249.000000  1.824900e+04        1.824900e+04        1.824900e+04   \n",
       "mean       1.405978  8.506440e+05        2.930084e+05        2.951546e+05   \n",
       "std        0.402677  3.453545e+06        1.264989e+06        1.204120e+06   \n",
       "min        0.440000  8.456000e+01        0.000000e+00        0.000000e+00   \n",
       "25%        1.100000  1.083858e+04        8.540700e+02        3.008780e+03   \n",
       "50%        1.370000  1.073768e+05        8.645300e+03        2.906102e+04   \n",
       "75%        1.660000  4.329623e+05        1.110202e+05        1.502069e+05   \n",
       "max        3.250000  6.250565e+07        2.274362e+07        2.047057e+07   \n",
       "\n",
       "       Extra Large Hass Avocado    Total Bags    Small Bags    Large Bags  \\\n",
       "count              1.824900e+04  1.824900e+04  1.824900e+04  1.824900e+04   \n",
       "mean               2.283974e+04  2.396392e+05  1.821947e+05  5.433809e+04   \n",
       "std                1.074641e+05  9.862424e+05  7.461785e+05  2.439660e+05   \n",
       "min                0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%                0.000000e+00  5.088640e+03  2.849420e+03  1.274700e+02   \n",
       "50%                1.849900e+02  3.974383e+04  2.636282e+04  2.647710e+03   \n",
       "75%                6.243420e+03  1.107834e+05  8.333767e+04  2.202925e+04   \n",
       "max                2.546439e+06  1.937313e+07  1.338459e+07  5.719097e+06   \n",
       "\n",
       "         XLarge Bags  \n",
       "count   18249.000000  \n",
       "mean     3106.426507  \n",
       "std     17692.894652  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%       132.500000  \n",
       "max    551693.650000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print descriptive statistics of these variables to see variable's magnitudes\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our variables have different magnitudes/scales. The minimum and maximum values of the variables are different. For example, the minimum and maximum value for the average price of avocados are 0.44 and 3.25, respectively. For small bags of avocados sold, the minimum and maximum values are 0 and 1.338459e+07, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AveragePrice range is:  2.81\n",
      "Total Volume range is:  62505561.96\n",
      "Small Hass Avocado range is:  22743616.17\n",
      "Large Hass Avocado range is:  20470572.61\n",
      "Extra Large Hass Avocado range is:  2546439.11\n",
      "Total Bags range is:  19373134.37\n",
      "Small Bags range is:  13384586.8\n",
      "Large Bags range is:  5719096.61\n",
      "XLarge Bags range is:  551693.65\n"
     ]
    }
   ],
   "source": [
    "# Get the range of numerical variables\n",
    "for col in['AveragePrice', 'Total Volume', 'Small Hass Avocado','Large Hass Avocado', 'Extra Large Hass Avocado', 'Total Bags',\n",
    "            'Small Bags', 'Large Bags', 'XLarge Bags']:\n",
    "    print(col, 'range is: ', data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranges of our variables are all different! \n",
    "\n",
    "# 2. Feature Scaling\n",
    "\n",
    "Models such as logistic regression, linear regression – or other models that involve a matrix – are very sensitive to different scales of input variables. If we use such data for model fitting, the result might end up creating a bias. Therefore feature scaling techniques are used before model fitting.\n",
    "\n",
    "As you can guess, feature scaling techniques change the scale of the variables. There are several ways how you can scale your features. In this notebook we'll demonstrate the **MinMaxScaling** technique that scales variables to their minimum and maximum values. scikit learn offers the `MinMaxScaler` class for this purpose. You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "\n",
    "The formula for min-max scaling is: \n",
    "\n",
    "**X_std = (X - X.min(axis = 0)) / (X.max(axis = 0) - X.min(axis = 0))**\n",
    "\n",
    "**X_scaled = X_std * (max - min) + min**\n",
    "\n",
    "- our Scaler subtracts the minimum value from all observations in our dataset and divide it by the range of values\n",
    "- it will transform each feature individually between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12774, 8), (5475, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split our dataset to training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Total Volume', 'Small Hass Avocado','Large Hass Avocado', \n",
    "                                                          'Extra Large Hass Avocado', 'Total Bags', \n",
    "                                                          'Small Bags', 'Large Bags', 'XLarge Bags']],\n",
    "                                                    data['AveragePrice'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit X_train with scaler: this computes and saves the minimum and maximum values \n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62505646.52, 22743616.17, 20445501.03,  1896149.5 , 19373134.37,\n",
       "       13384586.8 ,  5719096.61,   454343.65])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can access the maximum values using the .data_max attribute\n",
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([379.82,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can access the minimum values using .data_min attribute\n",
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and X_test with scaler and store it in the variables X_train_scaled and X_test_scaled\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [0.01381517 0.01317261 0.01469731 0.012413   0.01240248 0.01364267\n",
      " 0.00954372 0.00680544]\n",
      "Standard Deviation:  [0.05564355 0.05642417 0.05993686 0.05728216 0.05014797 0.05504383\n",
      " 0.04197211 0.0381179 ]\n",
      "Minimum value:  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Maximum value:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the scaled training dataset\n",
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this rescaling, all of the features have a range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [0.01310825 0.01220765 0.01382686 0.01118749 0.0122931  0.01354136\n",
      " 0.00940189 0.00691122]\n",
      "Standard Deviation:  [0.05431938 0.05368397 0.05637609 0.05521726 0.05263318 0.05735585\n",
      " 0.04421344 0.04079549]\n",
      "Minimum value:  [-4.72376194e-06  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "Maximum value:  [0.8365426  0.7508327  1.00122626 1.34295271 0.84625047 0.936923\n",
      " 0.75610389 1.21426513]\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the scaled testing dataset\n",
    "print('Mean: ', X_test_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_test_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_test_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the range of the features in the test set is not exactly 0 to 1. This is because `MinMaxScaler` has only been trained on the training data `X_train`, not on `X_test`, to prevent data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Imagine you've normalized the data using `MinMaxScaler` and delivered your work to the Senior Data scientist. He/she proposed you to scale the data using different scaling technique. The technique should transform the data such that its distribution will have a mean value of 0 and a standard deviation of 1. Find the right method [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Import the selected Scaling class\n",
    "\n",
    "# TASK >>>> Create a scaler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit X_train using scaler_technique\n",
    "scaler_technique.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([863900.62200485, 299592.80090027, 300493.93694457,  23536.90062314,\n",
       "       240274.93347659, 182601.48469704,  54581.44028887,   3092.00755128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the scaled values\n",
    "scaler_technique.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2470335 , -0.23307081, -0.24518142, ..., -0.24221355,\n",
       "        -0.22727127, -0.17853653],\n",
       "       [ 0.38575167,  0.95114752, -0.04266924, ...,  0.12612158,\n",
       "         0.38057064, -0.17697059],\n",
       "       [-0.24648986, -0.23332474, -0.24506811, ..., -0.24013427,\n",
       "        -0.22507195, -0.17853653],\n",
       "       ...,\n",
       "       [-0.09026841, -0.2298404 ,  0.12637791, ..., -0.12633798,\n",
       "        -0.22655094, -0.17853653],\n",
       "       [-0.24515317, -0.23325558, -0.2370544 , ..., -0.24730356,\n",
       "        -0.22490602, -0.17853653],\n",
       "       [-0.178008  , -0.22765843, -0.09848868, ..., -0.17222378,\n",
       "        -0.22040915, -0.17853653]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform X_train using scaler_technique and store it in variable X_training_scaled\n",
    "X_training_scaled = scaler_technique.transform(X_train)\n",
    "\n",
    "# Print X_training_scaled\n",
    "X_training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22530843, -0.22089906, -0.23209191, ..., -0.20010482,\n",
       "        -0.17750425, -0.17853653],\n",
       "       [-0.15102346, -0.13303044, -0.14183488, ..., -0.16818228,\n",
       "        -0.14337947, -0.17853653],\n",
       "       [-0.17910136, -0.13215362, -0.21735266, ..., -0.17069775,\n",
       "        -0.18266213,  0.35383824],\n",
       "       ...,\n",
       "       [-0.24478502, -0.23294502, -0.24112465, ..., -0.24335542,\n",
       "        -0.21291923, -0.17853653],\n",
       "       [-0.22290192, -0.23123422, -0.21616733, ..., -0.21513101,\n",
       "        -0.12119394, -0.15236067],\n",
       "       [-0.15747534, -0.05615708, -0.22350692, ..., -0.20481322,\n",
       "        -0.10136296, -0.17853653]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat the scaling also for X_test and store it in variable X_testing_scaled\n",
    "X_testing_scaled = scaler_technique.transform(X_test)\n",
    "\n",
    "# Print X_testing_scaled\n",
    "X_testing_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 1.55747586e-17  1.33497931e-17 -8.89986204e-18  3.55994482e-17\n",
      " -7.23113791e-18 -6.67489653e-18  7.78737929e-18  2.94807930e-17]\n",
      "Standard Deviation:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Print mean and standard deviations of X_training_scaled\n",
    "print('Mean: ', X_training_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_training_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Data source: \n",
    "\n",
    "Avocado dataset: https://www.kaggle.com/neuromusic/avocado-prices\n",
    "\n",
    "Data license: Database: Open Database\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
