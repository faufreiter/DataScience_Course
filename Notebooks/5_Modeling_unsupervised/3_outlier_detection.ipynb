{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"49889ed6-d538-4728-8710-b581715cc029","showTitle":false,"title":""}},"source":["## One-class SVM \n","\n","One-class SVM (one-class support vector machines) is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. It basically means that this algorithm is trained only on the 'normal' data. It learns the boundaries of these normal points and is therefore able to classify any points that lie outside the boundary as outliers.\n","\n","You can take a look at the parameters of the model down below. For more information regarding the model, please check out the [OneClassSVM documentation](\n","https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM\n",")."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2e3be2cd-ccd9-42aa-ab58-b572259ad69a","showTitle":false,"title":""}},"source":["class sklearn.svm.OneClassSVM(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"51b583da-c5db-4390-95f0-a85a0e03a39c","showTitle":false,"title":""}},"source":["#### Explanation of important parameters:\n","\n","- `kernel`: specifies the kernel type to be used in the algorithm.\n","- `nu`: the proportion of outliers you expect to observe .\n","- `gamma`: determines the smoothing of the contour lines."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"83bc4fb9-6644-428a-9cec-3917e8963cfd","showTitle":false,"title":""}},"source":["### One-class SVM Exercises\n","\n","First we will import `OneClassSVM` from `sklearn.svm`, `make_blobs`, `numpy`, and `matplotlib.pyplot`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c7fde8fd-da44-4e87-931e-c660c3008a74","showTitle":false,"title":""}},"outputs":[],"source":["from sklearn.svm import OneClassSVM\n","from sklearn.datasets import make_blobs\n","import numpy as np  # You will use np.quantile, np.where and np.random\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"31f085df-f5f5-4c77-bd42-76443ee23d72","showTitle":false,"title":""}},"source":["We have created a random sample data set below by using the `make_blobs()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"170012cb-9b7b-482b-88ff-deb9b6d6d229","showTitle":false,"title":""}},"outputs":[],"source":["np.random.seed(13)\n","x, _ = make_blobs(n_samples=200, centers=1, cluster_std=.3, center_box=(8, 8))\n","\n","plt.scatter(x[:,0], x[:,1])\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"601439a1-7f16-4dfb-b46d-d340ec767f22","showTitle":false,"title":""}},"source":["**TO DO:** We will use the rbf kernel type ([`radial basis function`](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)) for our model. Instantiate a new `OneClassSVM` model as `svm` with kernel type 'rbf', a gamma value of 0.001 and a nu value of 0.03."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"054f9fd8-475a-4591-af68-dad881aec1f5","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9a4b46c8-f851-4945-a4e6-0c65fbd2d926","showTitle":false,"title":""}},"source":["**TO DO:** Fit the model with the data set `x` that we created at the beginning and get the prediction data by using the `fit()` and `predict()` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"14e951e7-09b0-4e95-b34c-1efa11735b69","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dd943fb6-c8dc-40f0-9547-6812c627cca6","showTitle":false,"title":""}},"source":["**TO DO:** If everything has been done correctly before, you can now extract the negative outputs (where the prediction of the data is equal to -1) as the outliers.\n","Save these in these in the variable `values`. "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a88a8914-a9e0-4b23-b8df-d6a105442a79","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a515a9e1-3a0d-4c58-865b-45c7760e0457","showTitle":false,"title":""}},"source":["Now we will visualize what we have done by using `plt`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2fe16d09-f01d-4b3a-8a93-64d91f7049c4","showTitle":false,"title":""}},"outputs":[],"source":["plt.scatter(x[:,0], x[:,1])\n","plt.scatter(values[:,0], values[:,1], color='r')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c4733f5b-24c0-4c98-a9c5-06bd4b422884","showTitle":false,"title":""}},"source":["## Local Outlier Factor\n","\n","The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. Note that when LOF is used for outlier detection it has no `predict`, `decision_function` and `score_samples` methods. \n","\n","LOF is a score that describes how likely a certain data point is to be an outlier/anomaly.\n","\n","- When LOF is around 1 it is most likely that the data point is normal.\n","- When LOF scores higher than 1 it is most likely that the data point is an outlier.\n","\n","In general, the LOF of a point tells us the density of this point compared to the density of its neighbors. If the density of a point is much smaller than the densities of its neighbors (LOF â‰«1), the point is far from dense areas and, hence, an outlier.\n","\n","#### Explanation of important parameters\n","- `n_neighbors`: the number of neighbors considered\n","    - It should be greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster.\n","    - It should be smaller than the maximum number of close-by samples that can potentially be local outliers. \n","    - In practice, such information is generally not available, and taking `n_neighbors=20` appears to work well in general.\n","- `contamination`: the amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting, this is used to define the threshold on the scores of the samples.\n","\n","#### Explanation of attributes\n","- `negative_outlier_factor_`: the opposite of LOF for the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (`negative_outlier_factor_` close to -1), while outliers tend to have a larger LOF score. The local outlier factor (LOF) of a sample captures its supposed 'degree of abnormality'. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.\n","- `n_neighbors_`: the actual number of neighbors used for k-neighbors queries.\n","- `offset_`: the offset used to obtain binary labels from the raw scores. Observations having a `negative_outlier_factor` smaller than `offset_` are detected as **abnormal**. \n","\n","See more information here: [LocalOutlierFactor Documentation](https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html#:~:text=The%20Local%20Outlier%20Factor%20)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"412c501d-44c6-41be-84ce-82a5c7d9aedf","showTitle":false,"title":""}},"source":["### Local outlier factor exercises"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98ebedf8-299e-4f41-a815-e93d4ada81ea","showTitle":false,"title":""}},"source":["Firstly, we will important all necessary packages."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f82fee09-a058-432c-8f60-ad84f5a325b0","showTitle":false,"title":""}},"outputs":[],"source":["from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.datasets import make_blobs"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0ea6180e-4b8a-45cd-abf0-a4abc1e2e8a8","showTitle":false,"title":""}},"source":["We have created a random sample dataset below again by using the `make_blobs()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8f17f34f-0c1e-446f-a531-03e5d8c2b3e2","showTitle":false,"title":""}},"outputs":[],"source":["np.random.seed(1)\n","x, _ = make_blobs(n_samples=200, centers=1, cluster_std=.3, center_box=(10,10))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8700ff7d-c0f7-4f1b-af2c-523a4b8ee14e","showTitle":false,"title":""}},"source":["**TO DO:** Visualize the dataset in a plot."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"42702395-4051-48f8-b774-42ede1496a58","showTitle":false,"title":""}},"outputs":[],"source":["plt.scatter(x[:,0], x[:,1])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d2af10a6-2791-462f-a621-4630970fc4b6","showTitle":false,"title":""}},"source":["**TO DO:** Construct a `LocalOutlierFactor` model with `n_neighbors` set to 20 and `contamination` set to 0.03."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"09c68122-97b4-4833-86ba-5f11a032883c","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f293c66f-3c5a-43d8-abe0-3d814fbca13e","showTitle":false,"title":""}},"source":["**TO DO:** Fit the dataset which we generated in the beginning to the model and make prediction using the `fit_predict()` method."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"92db4b08-3022-426a-86b9-f9611c7b2b6e","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b717ccda-5258-41e4-9d03-16b608d3d086","showTitle":false,"title":""}},"source":["**TO DO:** Output the `negative_outlier_factor_` from the model."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ac378f8c-360f-4a4f-b955-9cd6d1203d6a","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d7fdf604-0c7f-4e42-ab00-b4a6ea73d08e","showTitle":false,"title":""}},"source":["**TO DO**: Assign the attribute `offset_` from the model to a variable called `threshold`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a964b925-5be2-4c2c-89ed-be980ae039ad","showTitle":false,"title":""}},"outputs":[],"source":["threshold = np.quantile(lof, .03)\n","print(threshold)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c5a5a2f7-c69d-4ec7-9ba8-db141f759995","showTitle":false,"title":""}},"source":["There are two ways that we can learn about outliers.\n","1. Using the `fit_predict()` method and extracting negative outputs as the outliers.\n","2. Obtaining the threshold value and extract the anomalies by comparing the values of the elements with the threshold value."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2a07f412-c027-4f00-b16b-15c46dddb22c","showTitle":false,"title":""}},"source":["#### Mehod 1\n","**TO DO:** Extract the negative outputs as the outliers and save them in the variable `values_1`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6bbd25b0-0a8b-44f3-8c58-d2d88cbb0c45","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b57557cd-459b-41eb-a5a8-bff5980fd4bf","showTitle":false,"title":""}},"outputs":[],"source":["plt.scatter(x[:,0], x[:,1])\n","plt.scatter(values_1[:,0],values_1[:,1], color='r')\n","plt.title (\"Method 1\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"094a8aff-3fa9-4c95-92bf-1953a0c90def","showTitle":false,"title":""}},"source":["#### Method 2\n","**TO DO:** Extract the anomalies by comparing with the threshold value and save them in the variable `values_2`.    \n","When the threshold value is bigger or equal to the local outlier factor score, this is regarded as an outlier. "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"73a76df5-fe13-430d-90cc-f945b145403d","showTitle":false,"title":""}},"outputs":[],"source":["# Task"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a1c2284f-8d77-4648-a3f5-fa174e85549a","showTitle":false,"title":""}},"source":["If everything has been done correctly before, we can visualize the outliers in the plot below."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"92f1a675-95f6-42ce-91a0-94e363be51ee","showTitle":false,"title":""}},"outputs":[],"source":["plt.scatter(x[:,0], x[:,1])\n","plt.scatter(values_2[:,0],values_2[:,1], color='r')\n","plt.title (\"Method 2\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5ec47d4e-9f3e-498a-84fb-d69c37beca22","showTitle":false,"title":""}},"source":["Some material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"3_outlier_detection","notebookOrigID":2649658360540809,"widgets":{}},"kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.8"},"vscode":{"interpreter":{"hash":"5d6ff9fa14007a600099b99274c7ab2ae18949db331e34153005375e00f74a95"}}},"nbformat":4,"nbformat_minor":0}
